[{"content":"","date":"14 September 2021","permalink":"/","section":"","summary":"","title":""},{"content":"","date":"14 September 2021","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"No fluff here, just a quick writeup of how to rapidly deploy a multimaster kubernetes cluster using K3OS on vSphere. This can probably be done even faster by remastering ISOs, using packer or cloud-init directly to the VM but that\u0026rsquo;s something for the future. K3OS is a minimalistic operating system intended to only run the minimal footprint required for K3S, which makes it incredibly compact, easy to deploy and manage. Another advantage is that K3OS can be updated through the kubernetes K3OS operating, which elimitates the overhead of having the manually manage an underlying operating system. For the people familiar with rancher, K3OS is the spiritial successor to rancherOS.\nPrerequisites # # Create prerequisites such as DNS names (not required but handy), IP addresses, etc. If you want you can use DHCP reservations which makes the configuration a bit simpler.\nYou also need a DHCP server to initially boot the machines so that they can get an IP address. I have a reserved range with an extremely short lease time (2 minutes) for this purpose.\nDownload the K3OS Iso from https://github.com/rancher/k3os/releases Create a VM in vSphere without any OS installed, the ISO attached and connected, and the correct network configured. I use 2vCPU/2GB RAM, 8GB Disk but this obviously depends on your use case. Do not boot it yet.\nCreate a template from this VM.\nCreate a cloud-init template file for your master with the following content and host them somewhere reachable over http/https. I\u0026rsquo;ve personally stored them in a github gist , which is obviously not ideal since it does contain sensitive information, but with the gists set to secret this is not an issue. Tip: If using github gists create a URL shortener using https://git.io as you will need to type in this URL manually. Make sure to also link the gist to the RAW gist and not the gist page itself.\n- github:srobroek write_files: - path: /var/lib/connman/default.config content: |- [service_eth0] Type=ethernet IPv4=172.21.3.3/255.255.255.0/172.21.3.1 IPv6=off Nameservers=172.20.11.9 SearchDomains=int.vxsan.com Domain=int.vxsan.com TimeServers=pool.ntp.org hostname: k3oscl01m01 k3os: k3sArgs: - server - --cluster-init - --tls-san=cl01-k3s.int.vxsan.com - --flannel-backend=host-gw Now we\u0026rsquo;ll go through this file section by section:\nssh_authorized_keys lets you set the ssh keys that are allowed to log in. I get my keys from my github profile but you can also manually enter keys here. This is critical as by default you cannot log in using a password either locally or using ssh. write_files sets the connman_config which is the initial network setup. Note that you can also set nameservers and ntp in the k3os section using dns_nameservers and ntp_servers, both options are honestly fine. The IPv4 settings are in the format IP/NETMASK/GATEWAY. The k3os section defines your K3OS config and determines how K3OS and K3S is set up initially (and on every reboot): On the master node we set --cluster-init which initialises an embedded etcd cluster. Ofcourse you can also deploy this using another backend such as mysql/mariaDB but if you\u0026rsquo;re at that stage you should be able to figure this out. --tls-san is utilised to generate a tls SAN certificate containing my loadbalancer IP address. If you are connecting directly to a node this is not required. --flannel-backend=host-gw is utilised as i was having some issues using the vxlan backend, most likely due to NSX-T being installed on the physical hosts. If you do not run this on hosts prepared for NSX-T/NSX-V you should leave this line out. Deployment # Once you\u0026rsquo;ve got your initial configuration set up, boot your first VM, and log in using the username rancher. Then once you\u0026rsquo;re logged in run the command sudo k3os install. Choose the option \u0026ldquo;install to disk\u0026rdquo; and select \u0026ldquo;yes\u0026rdquo; when asked to config the system with a cloud-init file. Enter the URL for your cloud-init file that you created earlier and after a short while your machine should reboot.\nNow you should be able to log in to your first master using ssh with the ssh key provided and the user rancher. If not, check if the network configuration is done correctly and redeploy.\nWhen you\u0026rsquo;re logged into your first node, you should be able to run the command kubectl get nodes and you should see a single node with 3 roles as follows. If not, wait a bit and try again, otherwise check the logs in /var/log/k3s-service.log\nk3oscl01m01 [~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION k3oscl01m01 Ready control-plane,etcd,master 127m v1.21.1+k3s1 You can also copy the kubeconfig to your local machine using scp, located in /etc/rancher/k3s/k3s.yaml. Note that you will have to adjust the hostname to point to your loadbalancer or your first node.\nYou will also need to retrieve your host token from the first node, which is located in /var/lib/rancher/k3s/server/node-token. Note that you cannot read this as a regular user, so you\u0026rsquo;ll have to use sudo: sudo cat /var/lib/rancher/k3s/server/node-token.\nNow for the other master nodes you will need to create a slightly different cloud-init file as follows. You will have to create a cloud-init file for each node unless you are using DHCP reservations, as the network details are different for each node. If you are using DHCP reservations, you can use the same cloud-init across your cluster.\nssh_authorized_keys: - github:srobroek write_files: - path: /var/lib/connman/default.config content: |- [service_eth0] Type=ethernet IPv4=172.21.3.4/255.255.255.0/172.21.3.1 IPv6=off Nameservers=172.20.11.9 SearchDomains=int.vxsan.com Domain=int.vxsan.com TimeServers=pool.ntp.org hostname: k3oscl01m02 k3os: token: fill_in_your_token k3sArgs: - server - --flannel-backend=host-gw server_url: https://172.21.3.3:6443 As you can see the initial config is still the same, but the k3os configuration is significantly different:\nthe server option is still set, as we want this machine to be a master. The token field is new here, and should be filled with the token retrieved from the first master as this k3s machine will join the cluster to the first master. The server_url field is set to the first master so that these machines can join the cluster. Note that you can also run this through a load balancer, but this requires a bit more setup with complicated health checks, so for my lab this is fine. Again, if you are not running on NSX enabled hosts you can leave the --flannel-backend option out. Repeat the above procedure of booting your machine from ISO, running sudo k3os install and pointing it to the cloud-init file. After a short while the node will reboot and you should be able to see an additional node appear in your kubectl get nodes.\nk3os: token: fill_in_your_token k3sArgs: - server - --flannel-backend=host-gw server_url: https://172.21.3.3:6443 As K3S does not by default taint its masters this should be enough to start deploying a management plane such as Rancher . However, if you want to deploy additional non-master nodes you can use the same cloud-init file and change the server k3Args to k3sArgs agent as follows:\nk3os: token: fill_in_your_token k3sArgs: - server - --flannel-backend=host-gw server_url: https://172.21.3.3:6443 ","date":"14 September 2021","permalink":"/posts/rapid-deployment-of-k3s-on-vsphere-using-k3os/","section":"Posts","summary":"No fluff here, just a quick writeup of how to rapidly deploy a multimaster kubernetes cluster using K3OS on vSphere. This can probably be done even faster by remastering ISOs, using packer or cloud-init directly to the VM but that\u0026rsquo;s something for the future.","title":"Rapid deployment of K3S on vSphere using K3OS"},{"content":"As part of a bit of a cleanup at home, i\u0026rsquo;ve decided to move all my services behind a single centralised loadbalancer, which at this point happens to AVI just so i can get more day to day experience with it. Now, in my previous setup i was using NGINX with some custom scripting to automatically request letsencrypt certificates, and since i\u0026rsquo;m quite happy with the letsencrypt service i wasn\u0026rsquo;t planning to change that. However, as AVI is obviously a different beast from NGINX, and the idea of having to manually renew certificates wasn\u0026rsquo;t exactly appealing, a new solution was needed. As it turns out, someone already adapted acme_tiny.py to be able to integrate with the certificate automation features of AVI, which can be found at https://github.com/avinetworks/devops/blob/master/cert_mgmt/letsencrypt_mgmt_profile.py , and Harold Preyers already wrote a blog post at https://www.comdivision.com/blog/how-to-request-lets-encrypt-certificates-on-the-nsx-advanced-load-balancer how to adjust this (thanks Harold!).However, as i - just as many people - only have a limited supply of public IP space available in my home lab, i was required to use AVI Enhanced virtual hosting, which doesn\u0026rsquo;t work with the script and blog post provided, as it assumes a VSVIP will exist for each service, which is not the case when using EVH, as all the services share a single parent VS. So, after some adjustments, the new script can be found at https://gist.github.com/srobroek/d71cbe93813bc45f4c580cc4d6d8090f , which adds a parameter to select the parent virtual service that is used for the HTTP-01 authentication used by letsencrypt. The complete steps to set this up the certificate generation are the same as in Harold\u0026rsquo;s blog, so i won\u0026rsquo;t be repeating that, but the EVH setup required some specifics.\nFirst off, create an EVH service for port 443 (or whatever port you want to run your services on). This is pretty straightforward, just select \u0026ldquo;Virtual Hosting VS\u0026rdquo;, Parent, and \u0026ldquo;Enhanced Virtual Hosting\u0026rdquo; for the virtual hosting type. You can also use SNI, but i prefer EVH as it\u0026rsquo;s simpler and more flexible.\nAfter this is done, you can set up your regular virtual services as normal by creating child virtual services. I won\u0026rsquo;t go through all the steps, but again create a virtual service, select \u0026ldquo;Virtual Hosting VS\u0026rdquo;, select \u0026ldquo;Child\u0026rdquo; and \u0026ldquo;Enhanced Virtual Hosting\u0026rdquo; and you should get a dropdown for the virtual hosting parent. Below that you\u0026rsquo;ll need to fill in your criteria to match, and note that if you are using EVH you are required to enter both a host match and a path match. To simplify things i\u0026rsquo;ve just created a string group matching \u0026ldquo;/\u0026rdquo; since most of my virtual hosts match on root, but you can use this if you want to run multiple services under the same FQDN using paths to differentiate instead.\nConfigure the rest of your service as normal, set up your pools, etc. and you should be good to go. Note that you are not configuring your certificates in the actual VS, as the parent VS is the one initiating the SSL connection, and it will select the correct certificate automatically based on the SNI header in the TLS handshake. Looking at the previous screenshot you can see all the certificates configured there, so that\u0026rsquo;s something to pay attention to. Also note that SAN certificates will work fine, i just prefer to have individual certificates for individual services.\nNow there is one last - but critical - task to perform: setting up the HTTP service. As letsencrypt handles the HTTP-01 authentication over unencrypted HTTP (to prevent circular dependencies, amongst others) we need to set up a service for port 80. Note that this does not need to be EVH, but i set it up as one regardless just in case i ever need virtual hosting on port 80. First off, set up your EVH as above, except with port 80 configured, and ensure that you use VIP sharing with the other EVH parent, as otherwise you won\u0026rsquo;t be able to use the same VIP.\nAlso note that this service does not have any child VS configured, nor does it have a pool set up, so you might be wondering how this actually works? If you go through the request script you will notice that it configures a temporary HTTP response rule that returns the letsencrypt well-known response when requested, so an actual backend pool is not required.\nThe last thing that i\u0026rsquo;ve configured which is not technically required but desirable is HTTP-\u0026gt;HTTPS redirection. Normally you\u0026rsquo;d use the Application profile in AVI for this, however as the .well-known traffic should never be redirected this unfortunately won\u0026rsquo;t work, so we\u0026rsquo;ll use HTTP request policies instead. In the policies-\u0026gt;HTTP request tab, simply add a rule with the following content:\nMatch: Protocol Type: HTTP Path: does not begin with (group letsencrypt) Action: Redirect: Protocol: HTTPS Port: 443 Status Code: 302 Note that i\u0026#39;ve used a string group, but since this is a one-off rule you can also just use a custom string with the contents \u0026#34;.well-known/acme-challenge\u0026#34;. This will ensure that all traffic is redirected to HTTPS, unless the path matches the ACME HTTP-01 challenge. And that\u0026#39;s all there\u0026#39;s to it, the rest of the process can be followed in the previously mentioned blogpost, and you should be good to go! ","date":"15 August 2021","permalink":"/posts/letsencrypt-certificate-automation-with-nsx-advanced-load-balancer/","section":"Posts","summary":"As part of a bit of a cleanup at home, i\u0026rsquo;ve decided to move all my services behind a single centralised loadbalancer, which at this point happens to AVI just so i can get more day to day experience with it.","title":"Letsencrypt Certificate automation with NSX Advanced Load Balancer"},{"content":"","date":"13 November 2020","permalink":"/tags/blogposts/","section":"Tags","summary":"","title":"blogposts"},{"content":"","date":"13 November 2020","permalink":"/categories/blogposts/","section":"Categories","summary":"","title":"blogposts"},{"content":"","date":"13 November 2020","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Since NSX was released, I\u0026rsquo;ve done a significant amount of work on distributed firewall designs, and over the years I\u0026rsquo;ve learned what does and does not work. The last 5 years when working with the NSX DFW have mainly been focused on a specific model of creating and managing firewall rules, the \u0026ldquo;consumer-provider\u0026rdquo; model, and in the last few weeks I\u0026rsquo;ve had the chance to optimise this and design it for micro-segmentation of an entire VCF environment, including some applications, so today i wanted to talk a bit about how this model works, its strengths and weaknesses, and why you might want to consider using this for your future micro-segmentation strategies:\nWhat is the consumer-provider micro-segmentation model? # The concept of the consumer-provider micro-segment is relatively straightforward, and is built on 4 main premises:\nfirewall rules always grant access to a single set of sources (the consumer) to a single set of destinations (the provider). Firewall rules are always service-specific. A specific firewall rule in this model will always grant you access to a specific service. Now this service can be available on multiple systems or even across environments, but you will generally never create a rule that allows all access to all services within an environment (with some exceptions). Firewall rule sources and destinations are always static as long as the rule does not change. Effective sources and destinations are changed through inclusion of members in the consumer or provider groups. This reduces the amount of changes required to your rule set to a minimum and reduces risks of misconfiguration. Consumer and Provider groups never directly include any members, instead membership is granted through membership of another group. this both promotes re-usability, keeps your group structure consistent and reduces the risk of misconfiguration. That said, how does the consumer-provider micro-segmentation model actually works? First off, let\u0026rsquo;s talk about rule structure: a rule in this model is always built in the following way:\nThe source is always a single consumer group, as is the destination a single provider group. The service is always a single service related to this rule, which can consist of multiple ports/protocols, but they all need to be related.\nIn addition, depending on the level of micro-segmentation that you are doing, you create a rule that allows intra-tier or intra-application traffic. Note that even this rule is based on a consumer-provider model (the application is both the provider and its own consumer in this case)\nThis will give you a rule set that can be seen below. For grouping i prefer to always keep application rules grouped by inbound rules (grouped by provider) since this seems more logical to me, but ultimately it made the most sense since you generally want to group by inbound traffic to reduce the amount of clutter in a ruleset.\nThe entire VCF and VVD ruleset can ultimately be condensed down to the following ruleset (excluding common infrastructure rules), which showcases some of the strengths of this model:\nSample VCF/VVD ruleset Now, let\u0026rsquo;s talk about grouping structure. Ultimately this is up to you, but what i like to do is the following:\nconsumer and provider groups have **groups ** as direct members. These groups can be structured in the following way:\nApplication groups: These are security groups focused on a specific application and - if required - a specific tier of the application. This can be achieved through multiple means, as described below. Segment groups: These are groups with segments as direct members, which can be used to allow an entire network access to a service, or as an alternative to application groups if you have per-application segments. Network groups: These are groups with direct IP members for a more broad access to services. Examples of this would be a network range for your end users, admins, internet access, etc. While segment and network groups are pretty clear, application groups might need some distinction. Application groups with direct IP members are relatively straight forward. For example, for your physical AD infrastructure you would create a group app.dc.ad.prod for your domain controller tier in your AD application in your production environment, which then contains the IP addresses of your specific domain controllers.\nTag based application groups is where the real power of this grouping lies. With tags, using the same example you can assign your DC VMs the following 3 tags:\nscope: environment, tag: production scope: application, tag: ad scope: tier, tag: dc and these tags can subsequently be used as a criteria for group membership.\nWhat this allows us to do is to tag VMs when provisioned in NSX and have them automatically gain access to a specific set of services, which can be done manually or through automation tools such as vRealize Automation, Terraform, Ansible, Puppet, etc.\nTo summarise the typical structure set of the consumer-provider model:\nConsumer group which _contain_Multiple Application groups which _contain_tag criteria or IP addresses or segments\nConsumer group which _contain_Multiple Application groups which _contain_tag criteria or IP addresses or segments\nFirewall rules use a single consumer and provider group as their source respectively their destination for a single set of common ports/protocols related to a that provider.\nConsumer group which *contain* Multiple Application groups which *contain* tag criteria *or* IP addresses *or* segments Consumer group which *contain* Multiple Application groups which *contain* tag criteria *or* IP addresses *or* segments Firewall rules use a single consumer and provider group as their source respectively their destination for a single set of common ports/protocols related to that provider.\nAn example of how this structure would work in practice can be seen below:\nConsumer-provider conceptual model Advantages and Disadvantages # So now that we know how all of this works, let\u0026rsquo;s talk about the why.\nFirst off, one of the major strengths of this model is the consistency. Your rulesets are pretty much going to be immutable, since you never have to touch your rules directly, only the indirect membership. This is a significant benefit for consistency, standard operational changes, documentation, and manageability of your firewall rules. Having a single immutable set of rules for an application that will always be consistent for the lifetime of that application is a massive benefit over standard firewall rules, and if that\u0026rsquo;s not entirely clear i would recommend drawing out the ruleset required for a complex environment with this methodology compared to standard rulesets.\nRepeatability becomes significantly simpler by using this model as well. If you have multiple copies of a specific application (for example, a generic 3-tier application with the same services), you can straight up create the new consumer/provider groups, copy the ruleset and apply it, which allows you to create actual ruleset templates of your applications instead of manually modifying rules every time. You could even automate this process through tools such as vRealize Automation or Terraform.\nEase of automation is another massive benefit of this system. Not only can we automatically include VMs in all the rulesets they require by tagging them upon provisioning, but the real power lies elsewhere: Security As A Service (As described below). By having a consistent set of consumer and provider groups, we can now easily add other applications to this group and have them consume another application without needing to know which rules need to be modified exactly. This is a massive benefit over directly automating firewall rules, as this is not only risky but also significantly more complex as you need to be aware of which rules have been created. It also allows for a perfect hybrid management model where firewall rules themselves are managed manually, but effective membership can be handled both manually as well as automatically.\nThat said, there are some disadvantages as well. For starters, the barrier to get started is higher than normal. You can\u0026rsquo;t just start writing rules and implement them, as you really have to think about your grouping structure first before even touching a firewall rule. This means that when you start with this model you need to be disciplined enough to apply it everywhere. Mixing this with normal rulesets is going to cause confusion, chaos, and ultimately something spontaneously selfcombusting. This is a highly prescriptive model that everyone needs to adhere to, which can be a good or a bad thing, but everyone in your organisation needs to understand this. The other disadvantage is related to visibility.\nDue to how NSX works, while you can see both effective members and membership definition of a group, you can not see how those effective members were added to a group. That means that you need to be very clear on how member groups were defined, and you need to have a consistent naming convention for all your groups. My personal preference (but this is obviously up to you) is the following:\nApplication groups: app.tier.application.environmentexample: app.collector.vrops.prod\nNetwork groups: net.description.environmentexample: net.admins.prod\nSegment groups: seg.description.environmentexample: seg.sddc.prod\nConsumer groups: consumes.service.tier.application.environmentexample: consumes.ldap.dc.ad.prod\nProvider groups: provides.service.tier.application.environmentexample: provides.ldap.dc.ad.prod\nThe reasoning behind this naming convention is that it allows 2 things:\nidentification of what a group is intended for simple group selection and creation in automation through consistent naming Obviously you can adjust this naming to your liking, but i would highly recommend that there\u0026rsquo;s consistent information between your consumer/provider groups and your application groups. If your provider groups care about the environment, include it in your application groups as well.\nAlternatives # There are ofcourse alternatives to this model, and over time i\u0026rsquo;ve worked on some of them. One example is tagging the VM directly with a consumes and provides tag, and including those directly into the group. The problem with this is that you run into scalability issues related to the maximum number of tags, and generally tagging is harder to automate than adding groups to another group in NSX. The other problem is that if you create a new service, you have to create new tags and potentially tag a lot of VMs, as opposed to just adding their application groups to the new groups.\nAnother alternative methodology is to directly add applications to the source and destination of a rule. While this absolutely works for simpler environments and applications with single destination groups, it can become a bit more confusing when we have multiple destination group. This would for example apply to the vrops endpoint rule, where we can add different application groups to be a https/ssh/snmp endpoint for vROPS. As you environment scales, you would have more and more groups in this rule to the point where it becomes unmanagable as your number of applications grows. In addition, it defeats the point of having an immutable ruleset if you still have to touch the source and destination groups. While having nested groups does add some overhead and complexity, the benefits of rule consistency and never having to touch your rules after provisioning far outweighs the complexity, especially as your environment grows.\nSecurity as a service # As mentioned above, one of the powers of this model is enabling security as a service. If you\u0026rsquo;ve ingested the above information, you should already have an idea why this model is so powerful for automation, but to reiterate, some of the benefits are:\nThe consumer and provider groups allow self-service for both publishing a provider and consuming a provider. By creating a rule for a service and allowing users to add their application group to the provider list of this service automatically, you can now allow application owners to publish their service in your microsegmentation model. In addition, you can allow users to subscribe to a published service by allowing them to add their application to a consumer group. As an example:\nJoe has a set of internal APIs that he wants to make available to the company, however due to the sentitivity of this application, he doesn\u0026rsquo;t want everyone to have access. In order to do so, Joe has the NSX admin create a firewall rule for his application and through automation adds his application groups to the provider group of this service. Now, whenever he creates a new service, all he has to do is request the new group to be added to the consumer.\nMatt wants to subscribe to Joe\u0026rsquo;s set of APIs. In order to do so, all he has to do is add his application groups to the consumer group of the rule created earlier, be it through automation during deployment time, a day 2 operation, a script or some other means.\nThe infrastructure team has a set of services they want certain systems to have access to, separately for both their production and development environment. When provisioning a new application, they can automatically add this application group to the consumers of either the production consumer group or the development consumer groups for this environment, depending on variables the user chose during provisioning.\nConclusion # While this is only a very high level overview of how this model works, once you get the hang of it it is incredibly simple and elegant in practice. It takes a bit of time to get used to the grouping structure and how to build the rulesets, but once implemented it becomes an incredibly powerful tool to keep your complex microsegmentation ruleset both manageable, functional and scalable.\nDon\u0026rsquo;t forget that this architecture described above is just an example of how to apply it for VCF and VVD management infrastructure, it can be adjusted to anything you like and it can absolutely serve as a baseline for your workload microsegmentation model. In addition, you can play around with basing rules on groups containing entire environments, groups spanning multiple applications of the same type (don\u0026rsquo;t forget, if your application tagging is sensible you can use \u0026ldquo;STARTSWITH\u0026rdquo; criteria to include all applications of a common type), or you can apply this to your federated cross-region distributed firewall to create a unified DFW policy across your global environment. This is only intended as a baseline architecture to follow, what you do with it is ultimately up to you.\n","date":"13 November 2020","permalink":"/posts/manageable-micro-segmentation-with-the-consumer-provider-architecture/","section":"Posts","summary":"This blogpost discusses how the consumer-provider microsegmentation model works, its strengths and weaknesses, and why you might want to consider using this for your future micro-segmentation strategies.","title":"Manageable micro-segmentation with the consumer-provider architecture"},{"content":"","date":"13 November 2020","permalink":"/tags/nsx-t/","section":"Tags","summary":"","title":"NSX-T"},{"content":"","date":"13 November 2020","permalink":"/categories/nsx-t/","section":"Categories","summary":"","title":"NSX-T"},{"content":"","date":"13 November 2020","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"In some recent engagements i\u0026rsquo;ve been involved in getting my customers started with a microsegmentations strategy, and one of the more obvious requests from people new to microsegmentation is to start with their infrastructure. Unfortunately, there is no real guidance out there for VCF + vRealize, and i found that a lot of the information is hard to find, and generally the rules that are available boil down to IP based rules. So in order to save myself and everyone else time in the future, i decided to build my own reference microsegmentation architecture for VCF + VVD, which is also well suited to be adjusted for customer specific applications. The document itself can be found at https://vxsancom-my.sharepoint.com/❌/g/personal/s_robroek_vxsan_com/EY7_Q0daSahPnWS_iSlqvSkBs7voTFNrv4nbuRBS-cJVlg?e=sRVMn5 , all of the information was retreived by reverse engineering the API behind https://ports.vmware.com (if you see me, ask me about that ;)). You are free to adjust and modify for your own use with attribution and respecting the Creative commons (Attribution-ShareAlike 4.0 International(CC BY-SA 4.0) license attached to this document.The framework provides 2 models: inter-app trust segmentation and zero trust microsegmentation. Pick whichever model is applicable to your situation, and look at the relevant traffic matrix. This contains all the required ports between different applications, your VCF infrastructure and any external services. Note that while the inter-app trust is relatively simple, the zero trust microsegmentation is significantly more complex to set up, manage, and troubleshoot. So choose wisely before going all \u0026ldquo;we must have zero trust microsegmentation!\u0026rdquo; gung-ho.Next, there is the ruleset. This ruleset is built on four main principles:\nRules are always defined as in-out inbound rules, unless an outbound rule is required (for example, when communicating with an external service. The reason for this is because it allows for grouping of rules in applications, creates a consistent model for rules and simplifies the ruleset in general. Rules are built on a consumer-provider model, with nested groups as the source and destination. Consumer and provider groups contain application groups, which can be defined as tagged VMs, tagged ports, or IP addresses. By using a consumer-provider model, you gain the following benefits: Adding or removing applications or scaling the environment does not require you to adjust the firewall rules, which reduces the risk of operator error. The model allows for simple automation as the only thing required to allow inter-application communication is to add a new or existing application to the security group. Application rules are consistent and can be templated for multiple versions or multiple environments. Each application ruleset is self-contained. All rules related to an application are stored in the same section, and no rules related to that application are stored elsewhere. This provides more consistency when managing or troubleshooting the ruleset. In addition, it increases DFW performance as it allows termination of the rule chain after hitting the application rules instead of going through the entire chain. Applications are defined using a model following service.role.application.environment. This allows for consistency in scripting, automation, reporting and generally rule configuration. By doing so, it becomes incredibly simple to create a new application for a different environment, add VMs to an application group, create a tagging strategy, etc. As you can see, the ruleset for zero trust is significantly more complicated. However, we\u0026rsquo;ll review both scenarios to understand what is going on here. Let\u0026rsquo;s take vRealize Network Insight as an example:\nIn this example, you can see that first off we have a rule that allows consumption of the HTTPS service on both the platform and the collector. We also have a rule for SSH, which goes to all the nodes since there is no need to split this up. In addition, we have a rule that allows netflow traffic to ESXi hosts, again defined as providers and consumers, however since netflow in VRNI is bidirectional as per the documentation for some reason, both the ESXi hosts and the collectors are simultaneously providers and consumers, allowing 2-way traffic. In addition, we see rules that allows the customer to define endpoints for SSH, HTTPS and SNMP, so that adding an external network device is as simple as creating an application group and adding them to the right providers group. The default intra-application rule allows unlimited traffic between the application components, and any other traffic is dropped. If a load balancer was involved, it would be added to the sources and destinations as well. Now let\u0026rsquo;s take a look at another application, vRealize Operations, but this time for zero trust segmentation.\nAs you can see, this ruleset is becoming significantly more complicated. We still have the consumer-provider model for services, however as vRealize Operations consists of multiple tiers, we\u0026rsquo;ve split all of them up into separate rulesets. As we do not need to follow the consumer-provider model here as the tiers are static within an application, we use the application groups to allow traffic between tiers and within a tier. Even with this complexity, the ruleset is still very repeatable and consistent between applications.\nApplications are grouped, as mentioned earlier by creating either consumer or provider groups, which contain application groups. Application groups are defined by IP addresses for non NSX backed services such as your vCenter, NSX Managers, but also external services such as DNS or NTP, or by tags. In order to allow the granularity for this kind of firewall model, we follow the following grouping strategy\nVMs are tagged with an environment tag. While this is not relevant now, it allows segregation of separate environments such as DMZ, Production, DTAP, etc. VMs are then tagged with an application they belong to, which allows us to either create a group for all components of the application or create more granular groups. If required, VMs are tagged with a role tag in order to allow segregation between application tiers. Note that it is not required that a VM belongs to a single role, in the case of vROPS often you will have your analytics nodes serving as data nodes as well. Load balancers are added done by including the autogenerated VIP group in NSX. Unfortunately i have not found a way to tag the logical port belonging to a load balancer as it seems they cannot be tagged through the UI. With that in mind, we now have a logical firewall model which allows for simple grouping of applications, adding multiple copies of the same application and segmenting them, scaling out applications or creating a multitenancy model, or simply implementing this as a default as part of your VCF or VVD projects. The ultimate goal would be that something like this excel sheet can be fed into a tool such as Terraform or PowerCLI for the standup of NSX microsegmentation, or it can be used in day 2 operations to automate your microsegmentation for both infrastructure as well as workloads. The current model only has two networks that allow access to tools such as vRA, WS1, VRLI, etc. but obviously you can model that however you want, such as limiting specific people to specific tools using AD groups, creating more granular access rules for DTAP environments, segregating your management infrastructure and SDDC infrastructure, you name it. By creating a consistent microsegmentation model you are ensuring that your ruleset will always be consistent and manageable. That said, some caveats\nThis list is far from complete, and a lot of ports for specific production functionality are not in here. This is by design, as it is inteded as a base ruleset for VCF and VVD. Examples are specific rules related to workspace one regarding mobile devices, app management, etc. as this is usually not in the scope of VCF. Products such as the Tanzu suite are not in the list simply to limit the complexity, but obviously this is all extensible. This design is not made for any kind of multisite deployment and currently does not take into account things such as using vROPS to monitor SDDC components outside of the management NSX environment. Until VCF supports federation, you will have to make due with IP addresses. A lot of choices in this design are based on personal experience and preferences. Some people like to use tags for everything, some people prefer to statically include VMs, they all have their advantages and disadvantages. Again, you are free to adjust this as you see fit, all i ask you that you credit the original source and honour the original CC license. This list is not guaranteed to be complete, it was built as a hobby project and has not been validated in an actual live environment, so keep that in mind when applying this. I have included a monitoring rule to both prevent people from shooting themselves in the foot as well as allowing you to monitor if any rules were missed. However, as there are no monitor rules for per-application, you could still lock yourself out of any NSX backed workloads. As usual, i am not responsible for any kind of downtime, datacenters burning down, interdimensional rifts, COVID-20 pandemics, election shenanigans or hurt feelings as a result of this information. Buyer beware. ","date":"28 October 2020","permalink":"/posts/vcf-microsegmentation-right-way/","section":"Posts","summary":"In some recent engagements i\u0026rsquo;ve been involved in getting my customers started with a microsegmentations strategy, and one of the more obvious requests from people new to microsegmentation is to start with their infrastructure.","title":"VCF Microsegmentation, the right way"},{"content":"","date":"24 April 2019","permalink":"/tags/vrealize-automation/","section":"Tags","summary":"","title":"vRealize Automation"},{"content":"","date":"24 April 2019","permalink":"/categories/vrealize-automation/","section":"Categories","summary":"","title":"vRealize Automation"},{"content":"A very brief one, but for a current project i\u0026rsquo;m working on LCM in a heavily locked down environment. So locked down in fact that even RDP to the servers is impossible, which makes making changing slightly impractical as everything has to go through the console.\nSo the first issue we ran into with regards to the prerequisites was setting UAC. Disabling UAC in the template was not enough, and UAC levels were being centrally enforced through a GPO, so as soon as the machine booted our setting disappeared as the registry key was overwritten\nAnother problem was adding the service accounts to local policies manually. As i mentioned, even RDP is not allowed, so going through each machine\u0026rsquo;s console becomes very tedious, especially when we want to deploy another vRA instance in the future.\nThe last issue was the actual sources. The way this customer installes roles and features is by attaching the ISO to the VM, installing the roles and features, and templating the VM. Now that obviously works for specific applications, but it\u0026rsquo;s a pain to manage and honestly i\u0026rsquo;d rather have LCM manage that instead of having separate templates.\nIn any case, all of the above prerequisites can fortunately be managed through Group Policy. We decided to use a vRA-specific OU containing all our machines, and by using the following GPO settings applied to that OU we can preconfigure all the manual prerequisites, without any manual reconfiguration required or any logging in to slow and frustrating VM consoles.\n- Computer Configuration - Windows settings - Security Settings - Local Policies - Security Options - User Account Control: Run all administrators in Admin Approval Mode - Set to disabled - User Rights assignments - Allow log on locally - Add your service account here - Log on as a batch job - Add your service account here - Log on as a service - Add your service account here - Administrative templates - System - Specify settings for optional component installation and component repair - Set to enabled - set Alternate source path to either the local ISO path (D:\\sources\\sxs) or a UNC path Don\u0026rsquo;t forget to extract your ISO to your UNC path if you decide to store your side-by-side folder centrally, otherwise just mount your ISOs to the machines before going through the prerequisites and you should be good to go.\nAlso do note that this does not actually log in to your VM and creates the profile, so you\u0026rsquo;ll still have to do that yourself, though i guess you could also automate that.\n","date":"24 April 2019","permalink":"/posts/vrealize-lifecycle-manager-vra-deployment-locked-down-environment/","section":"Posts","summary":"A very brief one, but for a current project i\u0026rsquo;m working on LCM in a heavily locked down environment. So locked down in fact that even RDP to the servers is impossible, which makes making changing slightly impractical as everything has to go through the console.","title":"VRLCM \u0026 vRA prerequisites in a locked down environment"},{"content":"","date":"22 October 2018","permalink":"/tags/powershell/","section":"Tags","summary":"","title":"Powershell"},{"content":"","date":"22 October 2018","permalink":"/categories/powershell/","section":"Categories","summary":"","title":"Powershell"},{"content":"Recently, a customer of mine needed to modify a large number of vRA reservations to facilitate a migration, and because there\u0026rsquo;s nothing wrong with being lazy, using powershell is always one of my go-to options. Using PowerVRA and some magic, instead of clicking through 200 reservations, we can reduce this process from a full day of work and running the risk of mistakes to a 10 minutes automated process.\nBecause Kyle Ruddy\u0026rsquo;s Beard told me so, and so i don\u0026rsquo;t forget, here\u0026rsquo;s the script in case someone can use it. With a bit of playing around, you could modify this to your own tastes, to use network reservations, add storage paths, etc. Note that there\u0026rsquo;s no serious error handling, the names have all been anonymized, and the script makes some assumptions about the names of the reservations, but with a bit of powershell knowledge you should be able to adjust it to your own environment.\nIn this case we\u0026rsquo;ve used a hashtable containing arrays of network names for each individual site, but obviously you can use whatever you want to store this info; CSV, JSON, plain text, XML (if you\u0026rsquo;re a heathen). The rest of the script is fairly straightforward. It loops over the hashtable, finds all reservations that contain the element key, loops over the reservations and then loops over the array contained in the hashtable to add each of them individually (if not already present).\n##Configure networks as an array for each individual site. To add new sites, add another element to the hashtable containing an array. To add networks to a site, expand the array. ##Assumption is that the reservations for a site have a name that contains siteNetworks.keys $siteNetworks = @{} $siteNetworks.SITE1 = @(\u0026#34;PGNAME1\u0026#34;,\u0026#34;PGNAME2\u0026#34;,\u0026#34;PGNAME3\u0026#34;,\u0026#34;PGNAME4\u0026#34;) $siteNetworks.SITE2 = @(\u0026#34;PGNAME5\u0026#34;,\u0026#34;PGNAME6\u0026#34;,\u0026#34;PGNAME7\u0026#34;,\u0026#34;PGNAME8\u0026#34;) $siteNetworks.SITE3 = @(\u0026#34;PGNAME9\u0026#34;,\u0026#34;PGNAME10\u0026#34;) ##Modify to suit your own environment. $vraTenantName = \u0026#34;tenant\u0026#34; $vraServerName = \u0026#34;vrava.domain.tld\u0026#34; ##Do not modify below here. ## Required for connectivity as powershell security is from the past. [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 connect-vraserver -server $vraServerName -tenant $vraTenantName ## Loop over site elements $siteNetworks.keys |% { $site = $_ $networkArr = $siteNetworks.Item($site) write-host \u0026#34;processing $site\u0026#34; ## Get all reservations matching *$site* $reservations = get-vrareservation |? {$_.name -like \u0026#34;*$site*\u0026#34; } |% { $reservation = $_ ## Magic sauce to get all networks already configured in this reservation $reservationNetworks = (($reservation.ExtensionData.entries |? {$_.key -eq \u0026#34;reservationNetworks\u0026#34;}).value.items.values.entries |? {$_.key -eq \u0026#34;networkPath\u0026#34;}).value.label write-host \u0026#34;modifying $($reservation.Name)\u0026#34; ## Loop over all networks in site hashtable elements $networkArr |% { $networkPath = $_ #check if reservation already has the specified network if ($reservationNetworks -notcontains $networkPath) { write-host \u0026#34;adding $networkPath\u0026#34; $reservation | add-vrareservationnetwork -NetworkPath $networkPath } else { write-host \u0026#34;$networkPath already exists in $($reservation.Name), skipping\u0026#34; } } } } ","date":"22 October 2018","permalink":"/posts/scripted-vra-reservations-powervra/","section":"Posts","summary":"Recently, a customer of mine needed to modify a large number of vRA reservations to facilitate a migration, and because there\u0026rsquo;s nothing wrong with being lazy, using powershell is always one of my go-to options.","title":"Scripted modification of vRA reservations with powerVRA."},{"content":"","date":"16 August 2018","permalink":"/tags/nsx/","section":"Tags","summary":"","title":"NSX"},{"content":"","date":"16 August 2018","permalink":"/categories/nsx/","section":"Categories","summary":"","title":"NSX"},{"content":"Note: As this is more of a reminder for myself so i don\u0026rsquo;t forget next time, don\u0026rsquo;t expect too much fluff.\nLast week i needed to demonstrate how to temporarily enable firewall rules using the NSX DFW for a customer use case that is pretty serious about zero trust policies. Specifically, both the VM and the guest OS required certain ports to be opened to access some administrative and deployment services, but during deployment of the system only. We have two workflows, one that adds a security tag to the machine, and another that removes it. Initially we used the buildingmachine POST and machineprovisioned PRE state to run the tag assignment workflow, and the machineprovisioned POST to remove it. However, as it turns out the machineprovisioned PRE state happens after the operating system has been sysprepped, which means it was too late for our use case.\nAfter some time looking in the documentation, it turns out there\u0026rsquo;s a OnCloneMachineComplete (for cloning) and OnCreatingMachineComplete (for basic virtual machines) event which can be used as the lifecycle event in the vRA EBS. I don\u0026rsquo;t have any screenshots to show you, but if you\u0026rsquo;re familiar with the EBS you shouldn\u0026rsquo;t have any issues finding it. Suffice to say, our machine was configured with a security tag directly after the VM object was created in vSphere before the first power on, and the tag was removed directly after installation in the POST Machineprovisioned lifecycle.\nUse this in whatever way you want, i\u0026rsquo;m sure people can come up with a lot of amazing use cases.\n","date":"16 August 2018","permalink":"/posts/running-pre-boot-vro-workflows/","section":"Posts","summary":"Note: As this is more of a reminder for myself so i don\u0026rsquo;t forget next time, don\u0026rsquo;t expect too much fluff.\nLast week i needed to demonstrate how to temporarily enable firewall rules using the NSX DFW for a customer use case that is pretty serious about zero trust policies.","title":"Running pre-boot, post-provisioning vRO workflows in vRA."},{"content":"It\u0026rsquo;s been a while since we\u0026rsquo;ve written anything about NSX-T, and - if hollywood is to be believed - every good story needs a sequel, preferably one that\u0026rsquo;s just a little less exciting than the first part. And do i have good news for you, because today we will be deploying a NSX-T lab environment based on NSX-T 2.1, with nested labs, and hopefully in the next part we\u0026rsquo;ll be expanding said lab to include an Arista leaf-spine topology using Arista vEOS.\nNow if you\u0026rsquo;ve read my previous NSX-T lab deployment blog post , you should know the basics of what NSX-T does and why it\u0026rsquo;s the best thing since mankind invented rocket surgery. In this post we won\u0026rsquo;t be going over the basics of NSX-T but we\u0026rsquo;ll try to give you a brief overview of the steps to take to deploy NSX-T in your lab environment the easy way, because if there\u0026rsquo;s one thing that has a learning curve for people used to managing vSphere and NSX-V, it\u0026rsquo;s T. I\u0026rsquo;ll also briefly go over the differences with previous releases of NSX-T and - if you\u0026rsquo;ve read my previous blogpost you\u0026rsquo;ll see why this story is going to be significantly less exciting. There\u0026rsquo;s less convincing antagonists, the story is more straightforward and - most importantly - there\u0026rsquo;s no exciting plot twist at the end.\nRemember, this is still applicable.\nThe lab overview # We\u0026rsquo;ll start our initial deployment simple, partially because of a lack of time on my side, and partially to reduce the complexity of the deployment. I know there are a large amount of people out there that just want to play with NSX-T and don\u0026rsquo;t really care for the nested network environment.\nOur lab looks something like the above. We have two vCenter servers, two ESXi clusters (these were all deployed through William Lam\u0026rsquo;s vGhetto script , thanks for that!). We are running vSphere 6.5 and have a standard deployment. All of this by the way is deployed on a single supermicro host, so if you want to deploy this yourself, don\u0026rsquo;t think you need additional network equipment. Obviously, you can start off with a single cluster as well. As you can see, the ESXi hosts have two interfaces, one for the management and one for their respective vTEP network. The same applies for the eve-NG host and the edge VMs, which are connected to both the management VLAN as well as the VLAN trunking port group.\nThe deployment # After firing off the vSphere lab deployment scripts , there are a few changes you have to make to make NSX-T work. First off, you need to increase the ESXi disk from 2GB to at least 4GB. The reason for this is that because - by default - the nested ESXi hosts are seriously constrainted in regards to memory, which means there isn\u0026rsquo;t enough space in the ramdisk used for scratch storage. What i did was increase the disk space, create a VMDK and set a persistent scratch drive. The details on how to set the scratch location can be found at https://kb.vmware.com/s/article/1033696 . Obviously, you could also temporarily increase the memory of the ESXi hosts, but that was something i only thought of later.\nAlso note that by default the vGhetto deployment scripts deploy a second nic (intended for NSX-V) which we will use for NSX-T. By default this is joined to the same portgroup as the portgroup for management, but we want it on a different port group. You can change the nic in the VM, or you can be lazy and use powershell:\nget-vm |? {$_.Name -like \u0026#34;*vesxi65*nsxt-1-*\u0026#34;} | Get-NetworkAdapter -name \u0026#34;Network Adapter 2\u0026#34; | Set-NetworkAdapter -NetworkName \u0026#34;VLAN15_VTEP\u0026#34; -Confirm:$false Obviously modify this for your own VM naming convention and port group names\nNow that you have a running vSphere deployment and several cups of coffee, it\u0026rsquo;s time to deploy the NSX-T manager. This is still done the same was as it was in NSX-T 1, basically perform the following steps:\nDeploy the NSX-T manager (also known as the unified appliance) OVA on your physical host. You could also nest it on the nested ESXi host, but keep in mind that you\u0026rsquo;d need to seriously up the resources in that case, and there\u0026rsquo;s no significant benefit in doing so. Next, deploy the controllers through the OVA, again on the physical host. You\u0026rsquo;ll need at least one controller, through three is the required for production. I\u0026rsquo;ve deployed three as i want to deploy as close to production as possible in this lab. Log in to the manager and each of the controllers over SSH (if you haven\u0026rsquo;t enabled it, you can enable it by going to the console of each individual VM and running set service ssh start-on-boot and start service ssh. next, log in to the manager and run the following: get certificate api thumbprint store this value somewhere. log into to each controller and run the following one by one (not simultaneously): join management-plane \u0026lt;ip-of-nsx-manager\u0026gt; username admin thumbprint \u0026lt;thumbprint\u0026gt; The thumbprint is the thumbprint you just got from the manager, and the CLI only accepts IP adresses, not hostnames. Keep that in mind. get control-cluster certificate thumbprint Again, store this thumbprint somewhere. On the manager run the following: get management-cluster status wait for all controllers to show up. Note that the control cluster status will show as unstable until it has been initialised. Then, on the first controller run the following: set control-cluster security-model shared-secret secret \u0026lt;some magical secret that is clearly not VMware1!\u0026gt; initialize controle-cluster get control-cluster status verbose wait for the results to show up and validate that the cluster is running. join control-cluster \u0026lt;nsx-controller2-ip\u0026gt; thumbprint \u0026lt;thumbprint\u0026gt; Repeat the above step for every non-primary controller in your cluster. Note that if you don\u0026rsquo;t have more than one controller you don\u0026rsquo;t need to do this. activate control-cluster get control-cluster status verbose Again, wait until your control cluster is stable. The NSX-T configuration # When you\u0026rsquo;ve deployed the manager, it\u0026rsquo;s time to log into the glorious HTML5 interface: It\u0026rsquo;s so shiny!\nWhen you\u0026rsquo;re done ogling the absolutely gorgeous NSX-T interface (i\u0026rsquo;m sorry NSX-V, but flash just doesn\u0026rsquo;t cut it anymore), we need to do some preparatory work.\nFirst, open the fabric tab and then \u0026ldquo;Transport Zones\u0026rdquo;. Click Add and create a new transport zone using the following settings:\nNames are up to you ofcourse. N-VDS mode should be set to standard, ENS is used for encrypted network services which we will not be touching upon. The traffic type should be set to overlay, as VLAN transport zones are primarily used for T0 edges to be able to route into the physical world.\nNext, open Inventory, group and then on top \u0026ldquo;IP Pools\u0026rdquo;. Add a new IP pool and add a subnet. Note that the CIDR field might be a bit confusing, as it is not actually asking for a subnet mask or classless notation but it\u0026rsquo;s expecting a full CIDR subnet. Don\u0026rsquo;t fall into the same trap as i did diving into the logs wondering if the UI is actually broken.\nIf you are familiar with NSX-V this should all be relatively comfortable to you, as the concepts are largely the same up to now.\nWhen the IP pool and TZ have been created, open the Fabric tab and click on \u0026ldquo;Compute managers\u0026rdquo;. Then, add a new compute manager:\nFill out the details as you\u0026rsquo;re used to, and don\u0026rsquo;t worry about the SHA256 thumbprint. If you leave it blank, when saving it will ask you to confirm the thumbprint as you\u0026rsquo;re used to from your usual vSphere deployments.\nOnce the compute manager is added, move over to \u0026ldquo;Nodes\u0026rdquo; (still in the fabric grouping). By default you won\u0026rsquo;t see any nodes as the dropdown is set to standalone hosts. Select the \u0026ldquo;Managed by\u0026rdquo; dropdown and select your vCenter. Now, put a checkmark next to your vCenter, click configure cluster and select the following options:\nEnable the \u0026ldquo;Automatically install NSX\u0026rdquo; and \u0026ldquo;Automatically Create transport zones\u0026rdquo;. Then, Use the transport zone you\u0026rsquo;ve just created, the default Uplink profile (you can create your own uplink profile which allows you to configure LAG, active/active profiles, MTU, etc. but for this lab the default uplink profile is more than enough). In addition, select \u0026ldquo;Use IP Pool for the IP assignment\u0026rdquo; and select the IP pool you created.\nFor the physical NICs field you\u0026rsquo;ll need to check what the name is of your vmnic in ESXi, but if you\u0026rsquo;ve used the vGhetto deployment scripts it should be one.\n*Note that if you get an error here when installing the NSX kernel modules, go back and read what i said about scratch disks in the start of this article. If you get an error on configuring the transport nodes (it will say \u0026ldquo;partial success\u0026rdquo;), doublecheck if the vmnic is actually correct.\nNow, once the configuration is complete, you should see the vmnic assigned to a NSX-T N-VDS switch as displayed below:\nOnce this is all done, you can start creating switches in NSX-T, and they will automatically be created on your ESXi hosts.\n.\nNote that we have only deployed in a single vCenter currently. As i want to use this lab for a PKS deployment, we\u0026rsquo;ve created separate transport zones for the separate pods. If you want to deploy both clusters to the same transport zone just repeat the above steps, except for the TZ and IP pool creation. If you want disjoint networks, repeat all steps above.\nAnd with that, we\u0026rsquo;re finishing off the first part of the deployment. By now, you should be able to provision VMs on your nested cluster and have them communicate within the same VNI. There\u0026rsquo;s no routing yet, which will be done in a followup post, including external connectivity.\nAlso, remember how i mentioned that a prequel is always slightly less exciting than the original? If we go through the original post i wrote, you\u0026rsquo;ll notice that the steps above to prepare the hosts and create the transport nodes was almost the length of this whole article, with multiple steps to take. ESXi agent installation was manual per host, transport node configuration was manual, everything was a manual step. Now, with NSX-T 2.1 we simply add the vCenter and everything is done automatically, like we\u0026rsquo;ve become accustomed to with NSX for vSphere.\nIn the next post - no ETA yet as i need to actually find time for these labs - we\u0026rsquo;ll be configuring the nested physical network, the edges and the uplink configuration. Until then, happy NSX\u0026rsquo;ing!\n","date":"25 May 2018","permalink":"/posts/building-a-nsx-t-nested-lab-part-2-electric-boogaloo-part-1/","section":"Posts","summary":"if you\u0026rsquo;ve read my previous blog post about NSX-T, you should know the basics of what NSX-T does and why it\u0026rsquo;s the best thing since mankind invented rocket surgery. In this post we\u0026rsquo;ll try to give you a brief overview of the steps to take to deploy NSX-T in your lab environment the easy way.","title":"Building a NSX-T nested lab part 2"},{"content":"note: header image shameless stolen from XKCD . Credit where credit is due\nOnce upon a time - as any good story is supposed to begin - people were working in IT. They were doing the same thing as you would in any other job, except with more computers and extremely liberal dispensement of coffee. At some point however, people working in IT diverged from what most people would call a normal job. And this is where our story begins..\nAbout 15 years ago, I started working in IT. First as a SMB sysadmin, then moving towards the University as a linux operator, onto more senior operations roles and project based work, and then into consultancy.\nI\u0026rsquo;ve been in the consultancy game for a while now, and as you go along you pick up contacts and meet people. Earlier this week i was talking to someone who was in consultancy as well, but in a completely different (non-tech related) branch. And during that conversation a thought just struck me; The uniqueness that is the social community built around IT and tech. Not just the VMware community, or the vExpert community, or any specific vendor\u0026rsquo;s social media marketing machine - the greater tech community as a whole.\nOver the last years i\u0026rsquo;ve met a ton of people through work and community related events, but an even larger amount of people i\u0026rsquo;ve talked to i\u0026rsquo;ve not met yet, and most likely the majority of the people i\u0026rsquo;ve talked to i\u0026rsquo;ll most likely never meet. Still, despite being thousands of miles apart, i\u0026rsquo;ve had intense communications with digital avatars representing actual live people on Twitter, IRC, Slack, LinkedIn, phone, email, you name it. I\u0026rsquo;ve helped - and been helped by - other people troubleshooting some obscure issues, presented webexes about some subject i\u0026rsquo;m passionate about, been supported by people i\u0026rsquo;ve only talked to over email with my VCDX certification, did reviews of other people\u0026rsquo;s VCDX designs and mentoring over email or phone, and communicated with a far greater amount of people than i would have in any other job.\nAnd it doesn\u0026rsquo;t just stop with your primary specialisation or job description. I\u0026rsquo;ve had twitter chats with people in completely different fields about a variety of subjects, from software development to IOT to home automation. We\u0026rsquo;ve even had a splitoff from the vExpert Slack community to form our own Slack centered around politics. The people in there are absolutely amazing, and i\u0026rsquo;d say i know them as well as i would most people i\u0026rsquo;ve actually met, even though they\u0026rsquo;re 8000 miles away, and 12 hours behind so we\u0026rsquo;ve got a completely mirrored day-night schedule.\nI\u0026rsquo;ve had help from so many people in the tech community, and learned more than i would\u0026rsquo;ve ever done by myself. So for that, thank you all for being such an amazing group of people from all professions and walks of life.\nBut as most stories actually have to end, i\u0026rsquo;d like to leave you with the closing thought that - even though sometimes we take these kind of things for granted - the technical community as a whole is truly unique and one of its kind. Everyone is willing to help eachother out, and we are all doing it for free, as everyone helping others out makes the community stronger as a whole I don\u0026rsquo;t think there are a lot of job profiles where you get to communicate with people around the world, both for work-related as well as personal subjects, and for me personally it\u0026rsquo;s one of the major factors of what makes tech so much fun for me.\n","date":"28 February 2018","permalink":"/posts/a-brief-thank-you-to-the-vcommunity/","section":"Posts","summary":"note: header image shameless stolen from XKCD . Credit where credit is due\nOnce upon a time - as any good story is supposed to begin - people were working in IT.","title":"A brief thank you to the tech community"},{"content":"","date":"28 February 2018","permalink":"/tags/personal/","section":"Tags","summary":"","title":"Personal"},{"content":"","date":"28 February 2018","permalink":"/categories/personal/","section":"Categories","summary":"","title":"Personal"},{"content":"","date":"7 February 2018","permalink":"/tags/ansible/","section":"Tags","summary":"","title":"ansible"},{"content":"","date":"7 February 2018","permalink":"/categories/ansible/","section":"Categories","summary":"","title":"ansible"},{"content":"With a fresh new year comes a fresh new set of VPS hosts, and as a resolution i\u0026rsquo;m planning to fully deploy these through ansible, along with migrating a lot of my existing machines to ansible as well.\nNow, as we all know, it\u0026rsquo;s generally considered a bad idea to store plaintext passwords for the world to see in a git repository, which is why ansible-vault exists. However, for ansible vault to work, we need to provide a password on the command line, and while obviously i could use the same password for all the vault files, i considered it a challenge to properly automate this and use a randomly generated password for each of my hosts and groups. For this, we\u0026rsquo;re going to use lastpass-cli . If you\u0026rsquo;re not aware of this tool, it is amazing and i use it to store literally everything in there.\nWhat i\u0026rsquo;ve done is store all my credentials sites in the following format:\nansible.vault.{{environment}}.type.{{varname}}: For example, a production host called vps01.transip.vxsan.com would have a lastpass entry called ansible.vault.production.host.vps01.transip.vxsan.com, while a group of webservers in development would have a vault entry called ansible.vault.development.group.webservers. Note that the entries are all empty except for the password and the name, as can be seen below:\nAlso note that you can create these entries through the lastpass-cli as well, so you could very easily make this an automated process through rake or various other deployment tools, even going so far as to generate a random vault password every time you add a new host or a new group.\nMy directory structure for ansible playbooks is as follows:\n- playbook - extensions (contains various scripts) - host_vars - host1.tld - app1 - app2 - appn - host2.tld - hostn.tld - group_vars - group1 - app1 - app2 - appn - group2 - plays (contains all playbooks) - roles (contains all roles) As you can see, because of the way host_vars and group_vars are structured, this setup allows for simplified automation when it comes to generating data either per host or per group.\nNow onto how we are using this:\nTo automate the ansible process we are using rake. An excerpt from my ansible rakefile using lastpass has been shown below:\nnamespace :ansible do def checklogin system(\u0026#34;lpass status -q\u0026#34;) end ansible_cli = \u0026#34;ansible-playbook \u0026#34; task :login, [:username] do |t, args| user = \u0026#34;#{args.username}\u0026#34; user.empty? and abort(\u0026#34;user not defined\u0026#34;) checklogin or system(\u0026#34;lpass login #{user}\u0026#34;) end task :run, :env, :play, :tag do |t, args| env = \u0026#34;#{args.env}\u0026#34; env.empty? and abort(\u0026#34;environment not defined\u0026#34;) play = \u0026#34;#{args.play}\u0026#34; play.empty? and abort(\u0026#34;play not defined\u0026#34;) tag = \u0026#34;#{args.tag}\u0026#34; checklogin or abort(\u0026#34;user not logged in\u0026#34;) system(\u0026#34;lpass sync\u0026#34;) Dir.foreach(\u0026#39;./group_vars\u0026#39;) do |item| next if item == \u0026#39;.\u0026#39; or item == \u0026#39;..\u0026#39; pwid = \u0026#34;vault.#{env}.#{item}\u0026#34; password=`lpass show -F #{pwid} --password 2\u0026gt; /dev/null` password.empty? or ansible_cli = ansible_cli + \u0026#34; --vault-id ../vaultid.#{item}\u0026#34; password.empty? or File.write(\u0026#34;vaultid.#{item}\u0026#34;, \u0026#34;#{password}\u0026#34;) password.empty? or File.chmod(0600,\u0026#34;vaultid.#{item}\u0026#34;) end Dir.foreach(\u0026#39;./host_vars\u0026#39;) do |item| next if item == \u0026#39;.\u0026#39; or item == \u0026#39;..\u0026#39; pwid = \u0026#34;vault.#{env}.#{item}\u0026#34; password=`lpass show -F #{pwid} --password 2\u0026gt; /dev/null` password.empty? or ansible_cli = ansible_cli + \u0026#34; --vault-id ../vaultid.#{item}\u0026#34; password.empty? or File.write(\u0026#34;vaultid.#{item}\u0026#34;, \u0026#34;#{password}\u0026#34;) password.empty? or File.chmod(0600,\u0026#34;vaultid.#{item}\u0026#34;) end tag.empty? or ansible_cli = ansible_cli + \u0026#34; --tag #{tag}\u0026#34; ansible_cli = ansible_cli + \u0026#34; -i ../#{env}.ini #{play} \u0026#34; Dir.chdir(\u0026#39;plays\u0026#39;) system(\u0026#34;#{ansible_cli}\u0026#34;) Dir.glob(\u0026#39;../vaultid.*\u0026#39;).each { |item| File.delete(\u0026#34;#{item}\u0026#34;) } end end Note that this temporarily writes the passwords to the cwd so in a shared environment so this could be considered insecure, even though permissions are set to 0600. However, as i am the only user on my VPS for my current setup it works well.\nAt the start, rake will check the user login status and request a login if the user isn\u0026rsquo;t already. Afterwards, for every host_vars and group_vars it will try to look up the corresponding password item from lastpass, write the password to a temporary file and use the file on the ansible run as a parameter. After the ansible run completes, the file will be removed again.\nObviously, this example is not limited to ansible (or Rake for that matter), and it will allow you to automate credential usage in your personal systems, lab environments and whatever you can think of. Personally, i already don\u0026rsquo;t know the majority of the passwords i use in my browser, and using lastpass on the CLI as well will reduce the amount of password reuse to a minimum there as well.\n","date":"7 February 2018","permalink":"/posts/ansible-vault-passwords-done-easy-with-lastpass/","section":"Posts","summary":"With a fresh new year comes a fresh new set of VPS hosts, and as a resolution i\u0026rsquo;m planning to fully deploy these through ansible, along with migrating a lot of my existing machines to ansible as well.","title":"Ansible-Vault password management made easy with Lastpass-CLI and Rake"},{"content":"As most of you are probably aware by now, i\u0026rsquo;ve moved to Dubai to work for VMware. By now, it\u0026rsquo;s been a bit over a week so i thought i might as well provide everyone back at home (or wherever you are) with an update.\n#Hit the ground running\nWhile obviously I was well aware that there\u0026rsquo;d be a lot to arrange, the migration process certainly isn\u0026rsquo;t going at a snail\u0026rsquo;s pace. Straight out of the airport - before even clearing customs - I was able to pick up my temporary work permit at the immigration office in the wee hours of midnight, allowing me to actually enter the country and work.\nAfter a few days of acclimatisation time (I entered the country amidst an especially humid week), the first few days of work consisted of forms, forms and more forms. The middle east sure loves their forms. While it\u0026rsquo;s becoming more and more then norm to register and manage things online, it\u0026rsquo;s not uncommon to send signed and stamped contracts, documents and forms - including your actual physical passport, which is still a bit of a strange sensation - back and forth with express couriers zipping accross the city on scooters delivering your forms as fast as they can. Kudos to the VMware MENA team (and Linn, a special thanks to you) who are already used to the process.\n#Paperwork, paperwork everywhere\nBefore the end of the first week i actually managed to get my medical and e-ID registration sorted, which - considering the amount of paperwork all the processes involve - is nothing short of impressive.\nA non-exhaustive list of the various procedures that require paper forms in one way or another:\nBank registration Visa registration Medical exam registration e-ID registration Eye test for the driver\u0026rsquo;s license Driver\u0026rsquo;s license registration Salary transfer permission form Despite all of this, the whole system seems to be built around this. Registering your e-ID and visa is literally a matter of going to the medical center, picking up your document in room 1, going to room 2 for the blood test, going to room 3 for the X-Ray, going to the next building for the e-ID registration, then back to room 1 to hand in all the forms. An entire production line of forms flying back and forth, if you will.\nThe same applies for the bank account. In less than 8 days after landing i\u0026rsquo;ve managed to get my bank account sorted, and now it\u0026rsquo;s just a matter of waiting for the cheque book and debit card. It took some pushing with the bank - as payroll needs the information before the end of the second week - but everything got sorted in time. A bit of corporate \u0026ldquo;wasta\u0026rdquo; \u0026quot; does certainly help in speeding up these kind of things.\n#WhatsApp\u0026rsquo;s the word\nThe thing i was already aware of through working for different clients here is that WhatsApp is huge. I\u0026rsquo;m not just talking \u0026ldquo;chat to friends and post random crap in WhatsApp groups\u0026rdquo;, but more \u0026ldquo;official communication from the bank regarding your account\u0026rdquo;. I had a relations manager from the bank introduce himself over WhatsApp because he didn\u0026rsquo;t have my email address yet. It\u0026rsquo;s not even considered that strange to send official documents over WhatsApp, though personally that\u0026rsquo;s something that makes me a tad nervous.. In this case i\u0026rsquo;m just glad i decided to get protonmail a long while ago, as the secure email functionality is certainly paying off. The idea of sending copies of official documents over WhatsApp or unencrypted mail is probably something that i\u0026rsquo;ll never really get used to.\n#Collect all the cultures\nSo even though i\u0026rsquo;ve had a decent amount of experience with the country, most of that was in Abu Dhabi, where cultural diversity was a given. But Dubai - compared to Abu Dhabi - has to be the one of the more interesting places when it comes to different cultures living and working alongside eachother. In the first week - besides the obvious English and Gulf Arabic - i\u0026rsquo;ve had people talking Hindi, , Lebanese Arabic, Italian French and even Polish just on the street and in the office. And if we\u0026rsquo;re talking food, as i was too lazy to cook i\u0026rsquo;ve ordered some takeaway from an Uzbeki restauraunt, which should hopefully be interesting..\nIn any case, it\u0026rsquo;s been a busy week, and most things have been sorted in a very expedite manner. The long wait is now for the e-ID, and we\u0026rsquo;re going to switch into high gear again as that will finally allow me to rent a place, get my driver\u0026rsquo;s license converted, and get my utilities sorted out.\n","date":"9 October 2017","permalink":"/posts/surviving-the-sandpit-week-1/","section":"Posts","summary":"As most of you are probably aware by now, i\u0026rsquo;ve moved to Dubai to work for VMware. By now, it\u0026rsquo;s been a bit over a week so i thought i might as well provide everyone back at home (or wherever you are) with an update.","title":"Surviving the sandpit - week 1"},{"content":"Currently i\u0026rsquo;m working on a NSX project involving a large amount of networks and a complicated firewall setup that needs to be migrated to NSX. Using tools such as vRealize Network insight for traffic flow insight helps a lot, but ultimately the DFW onboarding process needs to be SMART, and specifically the following:\nMeet a set deadline for each network as the network cannot be migrated before all firewall rules are created. Provide evidence that no firewall rules have been missed. Note: I strongly advise against involving any flammables in your DFW onboarding process as this may void your NSX warranty. What we are doing here is using the agile concept of burndown charts to map the progress of your distributed firewall onboarding.\nAs mentioned before, each network has a large amount of firewall rules, and to meet the above two objectives we need to migrate each network individually before moving on to the next.\nTo achieve the above two objectives, we are using a burndown chart to monitor firewall rule implementation, which would look something like this:\nUnfortunately i can\u0026rsquo;t show you a production chart due to restrictions from the customer, but the general idea should be clear: we\u0026rsquo;re showing the total hits on the distributed firewall fallthrough rule for a specific network over time, as well as the hits on specific rulesets for that network. As more and more rules are implemented, the orange line should hit 0 while the other lines should be growing.\nFor this purpose, we\u0026rsquo;ve created the following rulesets:\nFor each network, create a fallthrough rule which is applied to that logical switch, allows all traffic and logs with a specific tag containing the network ID and the firewall ruleset ID. An example of this tag would be DFW.NETID=123.RULESET=456.For fallthrough rules we are using the special ID \u0026ldquo;FALLTHROUGH\u0026rdquo; For each network, create a default rule that allows all inter-vlan traffic so that this traffic does not reach the fallthrough rule. For each network, create your firewall rules which are applied to that logical switch only and allows specific traffic to and from that network. This traffic is also logged and tagged with the network ID and the firewall ruleset ID. Now that we have these firewall tags we can easily filter on these properties in Log Insight. However, as we don\u0026rsquo;t want to manually query and filter every time we\u0026rsquo;re building a dashboard, we\u0026rsquo;ve created some extracted fields as can be seen below.\nFiltering the network ID\nFiltering the request ID\nNow that we have both these ID\u0026rsquo;s, we can start filtering. In Log Insight, we can now create dashboards that show a stacked graph for a specific network ID, grouped by request ID.\nNote that currently we are not blocking anything. We are allowing all traffic and are purely using the DFW to classify traffic and create these dashboards. The concept behind this is that as long as a network is not migrated, all traffic is filtered by the physical firewall. Whenever a rule is imported into the distributed firewall for a specific network, that traffic will start hitting that rule and not the fallthrough rule.\nOver time, more and more rules are added for a specific network and less traffic should hit the fallthrough rule for that network. At some point, the traffic to the fallthrough rule should hit zero and the fallthrough rule can be switched from allow all to deny all, at which point the rule migration is considered complete. To prove this in a management-friendly form, we are using a percentage-stacked graph over the last 30 days with the following filters:\ndfw_network_id = \u0026ldquo;network id of the migrated network\u0026rdquo;. dfw_rcec_id exists When the fallthrough rule for a network is switched, we can still use the logging to generate alerts. As there may be a few irregular but legitimate traffic flows which has not been imported (for whatever reason), this firewall rule will generate an alert so that an operator can inspect the flows and either create a rule or inspect the involved systems.\nSo by smart usage of vRealize Log Insight\u0026rsquo;s extracted fields and graphing options, we\u0026rsquo;ve managed to meet the above two requirements:\nshow the progress of the firewall rule through graphing the decrease of fallthrough rule hits. Show proof of firewall rule completion by monitoring fallthrough rule hits. When the migration is complete, the above tags aren\u0026rsquo;t obsolute. During production we can use t for a variety of purposes. Think of use cases such as:\nmonitoring if old firewall rules are still in use or can be safely removed. Sending alerts when a large amount of fallthrough rules occur. Monitoring changes to firewall rule requests (which should not happen without prior approval). All this could be done with normal queries in Log Insight, but using extracted fields makes it simpler, reproducable and allows providing the tags to different users and permits creation of multiple dashboards without having to rebuild queries every time.\n","date":"4 September 2017","permalink":"/posts/burn-down-your-dfw-onboarding-process-with-log-insight/","section":"Posts","summary":"Currently i\u0026rsquo;m working on a NSX project involving a large amount of networks and a complicated firewall setup that needs to be migrated to NSX. Using tools such as vRealize Network insight for traffic flow insight helps a lot, but ultimately the DFW onboarding process needs to be SMART, and specifically the following:","title":"Burn down your DFW onboarding with vRealize Log Insight"},{"content":"No personal data - outside of tracking cookies for personal stats monitoring - is processed on this site. This tracking data is processed by my personal statistics and analysis system and is for personal usage and optimisation of this website. Any personal information tracked- including IP addresses - is anonimised during processing and - as such - any statistics generated on individual or repeated visitors cannot be linked to an individual. No consent is requested nor required as this is most likely the least userfriendly thing invented since the cookie popup law. As all data and statistics are anonimised inline for processing there is no individual information stored or processed anywhere except during a brief moment in time where the IP address exists in a memory buffer.\nIf you would like to be excluded from tracking, please click here: https://stats.vxsan.com/index.php?module=CoreAdminHome\u0026action=optOut\u0026language=en . In addition, Do Not Track headers are fully respected.\nAny comments you post on articles here are processed and managed by disqus .\n","date":"17 August 2017","permalink":"/posts/privacy-policy/","section":"Posts","summary":"No personal data - outside of tracking cookies for personal stats monitoring - is processed on this site. This tracking data is processed by my personal statistics and analysis system and is for personal usage and optimisation of this website.","title":"Privacy policy"},{"content":"A fresh new ghost installation means a fresh new layout. It\u0026rsquo;s currently partially under development, but most features should be working. So if you have any feedback, bugs, items that aren\u0026rsquo;t working or any other criticism, i\u0026rsquo;d be glad to hear it. The only thing this site still really needs is a logo\u0026hellip; any suggestions?\nIt seems my DNS migration didn\u0026rsquo;t work as expected, so some people still got the old site\u0026hellip;\n","date":"1 August 2017","permalink":"/posts/new-layout/","section":"Posts","summary":"A fresh new ghost installation means a fresh new layout. It\u0026rsquo;s currently partially under development, but most features should be working. So if you have any feedback, bugs, items that aren\u0026rsquo;t working or any other criticism, i\u0026rsquo;d be glad to hear it.","title":"New layout!"},{"content":"","date":"31 July 2017","permalink":"/tags/vrealize-log-insight/","section":"Tags","summary":"","title":"vRealize Log Insight"},{"content":"","date":"31 July 2017","permalink":"/categories/vrealize-log-insight/","section":"Categories","summary":"","title":"vRealize Log Insight"},{"content":"For a project i\u0026rsquo;m currently working on we are using vRealize Log Insight as a logging and monitoring solution. One of the important requirements this customer has is the forwarding of syslog events to HP Arcsight, a SIEM solution. However, as the amount of syslog events is increased significantly with the addition of NSX and enabling the logging of the NSX DFW, we cannot filter and index all events on their existing HP Arcsight solution, as it would cause excessive strain on the systems. Because Loginsight can be scaled up with relative ease, we\u0026rsquo;ve instead decided to perform filtering and indexing of a variety of security events in vRealize Log Insight, before the events even reach HP Arcsight.\nOur production vRealize Log Insight environment is a relatively straightforward one. Each datacenter runs a cluster of 3 nodes with multiple ILB VIPs, which each perform mutual forwarding to eachother.\nFrom thereon we could forward all events directly to HP Arcsight through syslog. However, as mentioned before, due to the amount of events generated by vSphere and NSX (due to a large amount of DFW rules having logging enabled), we want to classify events which are interesting to the Security Operations team.\nTo achieve this, first we need to define our specific security events. An example of one of these events would be \u0026ldquo;Changing the syslog configuration for a physical router\u0026rdquo; or \u0026ldquo;Deleting the syslog configuration for a physical router\u0026rdquo;.\nAs the filtering options for forwarding in vRealize Log Insight are relatively simple, we\u0026rsquo;ll use extracted fields for this purpose.\nFirst, create a search to find your event. In our case, we purposely executed the use case, in our case changing the syslog server on a physical router and checked which syslog events were generated.\nNext, we\u0026rsquo;ll create an extracted field that matches our exact use case:\nHighlight any text in the syslog message, and the filter menu should pop up. Click \u0026ldquo;Extract field\u0026rdquo; and you should see a custom field definition on the right.\nNow, we need to create our custom field. One thing to keep in mind here is that some use cases may be matched by multiple commands. In the case of a router, due to the way command completion works, one could set the syslog configuration with both set system syslog as well as the slightly more obscure s sy sy command, or even s sy sy, to obfuscate any matches that have a fixed space count.\nFor example, our matching regular expression, and the extracted value would be as follows:\nse?t?\\s+(sy)s?t?e?m?\\s+(sy)s?l?o?g?(.*)$\nwhich matches with everything from the moment a user ran the \u0026ldquo;set system syslog\u0026rdquo; (or any shorthand) command, and which is also what our security operations team is mostly interested in. For the people interested, the rules of the regex above are:\nMatch exactly 1 instance of the character \u0026ldquo;s\u0026rdquo; Match 0 or 1 instances of the characters e, t Match 1 or more whitespace characters Match the string \u0026ldquo;sy\u0026rdquo; Match 0 or 1 instances of each of the characters s, t, e, m Match 1 or more whitespace characters Match the string \u0026ldquo;sy\u0026rdquo; Match 0 or 1 instances of each of the characters s, l, o, g Match any characters Match the end of string. note that it seems that Log Insight always matches case insensitive, so we did not need to worry about that in our above regex\nIn addition, in some cases we do not wish to match this use case on every event that would match this specific regular expression, as it could pollute the SIEM event stream and potentially generate false positives. To do so, we set an additional context on \u0026ldquo;hostname\u0026rdquo; and have it match a regular expression on the format of the router hostnames. In our case, the hostnames for the physical routers follow the following pattern:\nCountry Code City Code Site Number the string \u0026ldquo;FRW\u0026rdquo; A 4 length Device Number So in our case, we can match the hostname field with the following regular expression:\n(CC1|CC2|CC2)(CITY1|CITY2|CITY3)[1-9](FRW)[0-9]{4}\nWhen we store this extracted field for all users, this will now add an additional field to each event that matches exactly this use case. While vRealize Log Insight cannot filter forwarding based on dynamic fields, we can still provide this field to the SIEM team to reduce the amount of indexing and filtering that needs to be done. They only need to look at the existence of a specific field to link it to a specific use case, and can even directly ingest the contents of this extracted field as the relevant data without having to filter the message themselves.\n","date":"31 July 2017","permalink":"/posts/vrli-preclassify-filter-events-for-siem/","section":"Posts","summary":"For a project i\u0026rsquo;m currently working on we are using vRealize Log Insight as a logging and monitoring solution. One of the important requirements this customer has is the forwarding of syslog events to HP Arcsight, a SIEM solution.","title":"vRealize Log Insight : pre-classifying events for SIEM"},{"content":"Most of you that are reading this will either have their own lab, or a corporate lab environment, and - most likely - regularly install a variety of vendor products to try out new products, test new features, create demo environments for customers, study for certifications, and more.\nAnd what is the thing each and every one of those environments have in common?\nThat\u0026rsquo;s right. Unsigned certificates everywhere. Now if you\u0026rsquo;re just testing products it\u0026rsquo;s mostly an inconvenience, but if there\u0026rsquo;s one thing i can\u0026rsquo;t stand it\u0026rsquo;s customer demos with self-signed certificates.\nNow obviously you can import each and every one of the self-signed certificates, but that\u0026rsquo;s mostly inconventient, risky and hard to manage, so why not set up your own PKI?\nNormally, setting up a full-fledged PKI can be quite complicated, so i\u0026rsquo;m going to show you how you can set up a simple PKI in less than 15 minutes using nothing but a single linux server.\nFirst of all, start off with deploying a linux server (or use one you\u0026rsquo;ve already got) and make sure git and openssl are installed. Next, download OpenVPN\u0026rsquo;s easy-rsa through the following command\ngit clone https://github.com/OpenVPN/easy-rsa\nChange into your cloned directory and the subdirectory easyrsa3 (cd easy-rsa/easyrsa3) and run the following commands:\nBuilding the root # ./easyrsa init-pki ./easyrsa build-ca\nEnter a passphrase for your CA certificate and store it somewhere securely as you\u0026rsquo;ll need this later. This will initialise your PKI infrastructure and generate a root CA certificate.\nNote: if you want an offline root CA and an intermediary signing CA you can run the command ./easyrsa build-ca subca to generate a CA certificate and signing request which you can sign with your root CA just as we will be doing in the rest of the tutorial below. For now, we\u0026rsquo;ll just focus on a 1-tier PKI.\nGenerating the request # Depending on what system you want to sign certificates for, you can either run this command on the CA server (for appliances that can\u0026rsquo;t generate their own requests) or on a different linux system).\nIf you\u0026rsquo;re requesting the certificate on the CA, you don\u0026rsquo;t need to do anything else. Otherwise, install easy-rsa on your client system as described above. Then, run the following command either on the CA or on the client:\n./easyrsa gen-req alias\nwhere alias is the name under which you want to store it in the PKI. Personally i prefer to use the FQDN of the system. This will ask you for the common name, as well as a passphrase. this should be different from your CA passphrase for obvious reasons!\nWhen ran on a client, copy it to a location accessible by the CA and run ./easyrsa import-req /path/to/cert.req alias. This imports it into the PKI.\nNext, sign the request:\n./easyrsa sign-req server alias\nEnter your CA passphrase and your certificate has been generated. You can find the certificate in ./pki/issues/ under the alias you\u0026rsquo;ve given at request time. Your private key can be found under ./pki/private/ under the same alias.\nIf you need a full chain you can generate it as normal by concatenating the certificate files together. If you need to convert your private key to a RSA private key (required for quite a few tools such as NSX loadbalancers and vRealize Automation) , you can use openssl rsa -in /path/to/private/key which will convert it to a RSA format.\nNow the only thing you need to do on your systems is to import the root certificate (found under ./pki/ca.crt) and you\u0026rsquo;ve got fully signed and validated certificates in under 15 miutes! Finally you won\u0026rsquo;t have an excuse to use unsigned certificates anymore, and you can endlessly mock those that do.\n##Advanced features\nAs i\u0026rsquo;m running a vRA distributed lab i am required to use SAN certificates. As you may have noticed, easy-rsa only generated single CN certificates by default. But fortunately, through some simple commands you can extend your certificates with any features openSSL supports.\nFor example, to generate a SAN certificate for a clustered vRealize Automation deployment, run the following before generating the request:\n/easyrsa --subject-alt-name=\u0026quot;DNS:host1,DNS:host1domain.tld,DNS:lbhostname,DNS:lbhostname.domain.tld\u0026quot; gen-req alias\nAnd that\u0026rsquo;s it!\nSome other advanced features and customization options can be found in https://github.com/OpenVPN/easy-rsa/blob/master/doc/EasyRSA-Advanced.md , including setting various certificate flags, expiration times, key lengths, and more.\n","date":"12 July 2017","permalink":"/posts/setting-up-your-own-pki-the-simple-way/","section":"Posts","summary":"Most of you that are reading this will either have their own lab, or a corporate lab environment, and - most likely - regularly install a variety of vendor products to try out new products, test new features, create demo environments for customers, study for certifications, and more.","title":"Setting up your own PKI - the simple way"},{"content":"","date":"12 July 2017","permalink":"/tags/technology/","section":"Tags","summary":"","title":"Technology"},{"content":"","date":"12 July 2017","permalink":"/categories/technology/","section":"Categories","summary":"","title":"Technology"},{"content":"For those that regularly read XKCD, this one may be familiar for you:\nLet me start from the beginning. My previous employer OGD has a yearly event called \u0026ldquo;Technival\u0026rdquo;, which was all about a wild variety of geeky things organised and built by their employees, and the Virarium was the brainchild of me and Robbert Erents . You can see us handling the environment with extreme caution in this charming press photo:\nBack then, we were just trying things and the project wasn\u0026rsquo;t a great success due to a variety of factors: The anti-malware software we used was working too well (it blocked execution of malware even though we told it to just detect.), the project was built on a heap of PowerCLI scripts to deploy and destroy VMs, and ultimately: there isn\u0026rsquo;t a lot of malware left these days that actually spreads peer to peer. But in the end, we had a ton of fun building and designing the environment.\nAnd now with SHA2017 around the corner, the time is ready for Virarium 2.0, new and improved. I\u0026rsquo;ve had a lot of time to think how to actually design and implement this properly, and 4 years of experience with automation helps a lot in this.\nSo the way the Virarium 2.0 is planned is as follows:\nVirtual machines are managed by VMware vSphere with NSX. This allows us to use hypervisor based IDS tools which do not interfere with the VM, and it allows us to tag Virtual machines based on the operating system, the applications running, the security posture, you name it. We\u0026rsquo;re looking at Palo Alto\u0026rsquo;s PANOS since it has out of the box dashboards for vRealize Log Insight. Virtual machines will run operating systems and applications in a wide variety, from purpose-built vulnerable machines such as Metasploitable to Windows 2016 and Honeypots and will be classified based on NSX security tags. All data generated by NSX and IDS tools log to vRealize Log Insight which provides a structured format for unstructured data. vRealize Orchestrator regularly pulls the log data in a structured format from Log Insight through the API and will use this data to \u0026ldquo;score\u0026rdquo; VMs based on information such as threat scores, counts of IDS events, you name it. vRealize Orchestrator will in turn use this information to decide the health state of the virtual machines and will - on a scheduled cycle - kill off VMs and provision healthy new ones. This can be as simple or as complex as we want to make it, since we have all the security information on the VM and can apply any kind of weighing mechanism to the provisioning decisions. vRealize Orchestator can push events back to vRealize Log Insight through the ingestion API which allows us to store even more information on the VMs in a structured format. vRealize Log Insight will turn this into pretty graphs, counters, gauges, and scoreboards to give people something pretty to look at. Our basic vRO workflows currently consist of the following:\nSome mockups of what this would look like:\nCurrently the main work is in the vRO workflows, getting the provisioning done correctly, and getting the correct threat events from Palo Alto Panorama as soon as i can get my hands on a NFR license. After that, the main work will be in creating fancy dashboards, and obviously to get some friendly hackers at the SHA2017 conference to purposely infect and try to exploit our malware ant farm.\nWhile this may not have been a very indepth blogpost, we\u0026rsquo;re currently working on getting all the tooling correctly configured and building workflows, and i\u0026rsquo;ll post an update as soon as i have more information to share.\n","date":"14 May 2017","permalink":"/posts/building-the-virarium-or-you-know-normal-people-have-aquariums-part1/","section":"Posts","summary":"For those that regularly read XKCD, this one may be familiar for you:\nLet me start from the beginning. My previous employer OGD has a yearly event called \u0026ldquo;Technival\u0026rdquo;, which was all about a wild variety of geeky things organised and built by their employees, and the Virarium was the brainchild of me and Robbert Erents .","title":"Building the Virarium 2.0 - or \"You know, normal people just have aquariums.\" - Part 1"},{"content":"Since my last tweet and blogpost on a bug in NSX-T when deploying on a nested ESXi host , i\u0026rsquo;ve had a few requests from people to describe the actual lab setup used, the procedure and a quick-and-dirty guide to get started with NSX-T on vSphere. This blogpost will focus on the actual physical lab setup, the virtual lab setup, the physical (and virtual) networking, and the actual NSX-T implementation and setup.\nNote: This blog presumes the reader has decent skills in networking and virtualisation, as we\u0026rsquo;ll be going full-on inception on our virtualised network. If you\u0026rsquo;re having issues wrapping your head around nested labs, you\u0026rsquo;re in for a ride.\nDue to the size of this blogpost a table of content has been provided.\nIntroduction to NSX-T Physical lab setup Virtual network setup NSX-T deployment Preparation Deploying the NSX-T Manager and controller Prepare and configure hosts Prepare transport zones and transport nodes Edge host deployment and configuration Configure logical routing and uplinks Configure upstream routing Configurations #Introduction to NSX-T\nFor the people not familiar with NSX-T (also known as \u0026ldquo;Transformers\u0026rdquo;, hence the spiffy product logo, and not to be confused with the Honda NSX-T), NSX-T is a new product from VMware, based on concepts from both NSX-v as well als NSX-MH (short for Multi-Hypervisor). As most people will mainly be aware of NSX for vSphere (which is what most people refer to when they talk about VMware NSX), NSX-MH was the Nicira implementation for non-vSphere platforms. While it was comparable to NSX for vSphere in some aspects, the intended user base, feature set, configuration and deployment was completely different.\nI too have always dreamed about a network that transforms into a car. Or a car with Layer 3 routing capabilities. Now, with NSX-T we see a convergence of vSphere-based and non-vSphere based SDN into a single platform. While NSX-T is an entirely new product and technical comparisons cannot be made directly between NSX-T and NSX-V or NSX-MH, a lot of concepts from both NSX-v and NSX-MH have been kept and function similarly from a conceptual point of view. While feature parity is not there yet, NSX-T offers some significant benefits over NSX-v such as control plane based routing configuration, improvements to redundancy (BGP-MED and BFD come to mind), decoupling of the vCenter management layer, a significantly better API, and much more. While you\u0026rsquo;ll mostly see NSX-v in your typiscal vSphere based datacenter environment, don\u0026rsquo;t be surprised if you\u0026rsquo;ll start hearing the name NSX-T much more in the coming year.\n#Physical lab setup\nMy physical lab consists of the following components:\na Supermicro server with the following specs: 6 core Xeon-D with hyperthreading 128GB Memory 1TB Samsung 960 Pro M.2 NVMe 4x 4TB WD Red 5400RPM 2x 1GbE and 2x10Gbe (note that these are only running at 1GbE speeds because of the switch). A second supermicro server with a 128GB SSD, a 4 core CPU and 32GB of memory, which is mostly used if i need some physical hardware to mess around with things. a Ubiqiuiti edgeswitch lite 24x1GbE switch. a Ubiqiuiti edgerouter lite providing WAN connectivity and services such as firewalling and dynamic routing. On this box we run ESXi 6.5 with a vCenter server and all other related infrastructure components such as AD, NSX-v, FreeNAS, etc.\nL3 connectivity is provided by the Edgeswitch Lite, with the exception of some VLANs such as DMZ or transit VLANs which are L2 up to the Edgerouter Lite. A quick overview of my network infrastructure at home can be seen below.\nAt the distributed vswitch level we\u0026rsquo;ve created the following relevant port groups:\n2 Trunk port groups which trunk all VLANs in the ranges 23xx respectively 24xx, plus the 10xx range. These are used to trunk traffic to our nested ESXi hosts. 1 Trunk port group which trunks all VLANs in the range 10xx. This is used to connect our virtual switches running in eve-NG to the physical network. Access port groups for transit networks, which will be used to connect the NSX Edges to the physical network. For the purposes of this lab, we are running the following components on this machine:\n2 Nested ESXi hosts with the following specs: 4 virtual Nics connected to their \u0026ldquo;site-local\u0026rdquo; trunk ports as described above (either 23xx or 24xx). 2 Nics have been assigned to the standard interfaces (management, vMotion, storage, etc.), while two have been left unassigned. This is important for later. 2 vCPUs 12GB Memory 8 GB local storage Connected to a Freenas provided iSCSI datastore 1 Virtual machine running Eve-NG 2 virtual nics, 1 connected to my management VLAN, 1 connected to the 10xx trunk port. Eve-NG allows you to run virtualised network components from pretty much every major vendor inside a virtual machine, providing you with nearly all of the features that do not require hardware support.\nThese ESXi machines were then connected to the vCenter and placed in their own cluster each. Note that i would normally use William Lam\u0026rsquo;s vGhetto nested lab deployment , but for this lab i wanted to build the machines myself.\nSo the structure we\u0026rsquo;re getting in vCenter is as follows:\nvCenter Cluster \u0026ldquo;Nested - Site 1\u0026rdquo; vesxi101.int.vxsan.com (Host) Nested VM for testing purposes Cluster \u0026ldquo;Nested - Site 2\u0026rdquo; vesxii201.int.vxsan.com (Host) Nested VM for testing purposes Cluster \u0026ldquo;Physical\u0026rdquo; pesxi101 vesxi101.int.vxsan.com (VM) vesxi201.int.vxsan.com (VM) If you\u0026rsquo;re getting confused right now, consider the fact that you can leave the physical server and vCenter components out of the equation for now. They\u0026rsquo;ll be purely used to deploy workloads on, but this will be no different than if you were to deploy all remaining components on the nested ESXi hosts.\nVirtual network setup # So earlier in this blog post i told you we\u0026rsquo;d be going full-on inception? This is where the fun really starts. Remember the Eve-NG machine we\u0026rsquo;ve deployed earlier? We\u0026rsquo;re running two arista vEOS QEMU images on there which are connected to the VLAN 10xx trunk port connected to the Eve-NG VM. This means that all VLAN tagging passed through the Eve-NG VM arrives at the Arista vEOS images untouched, which allows us to do VLAN trunking all the way at the virtual switch level.\nLogically, this looks something like this:\nConceptually, it looks something like this:\nWhile the whole setup procedure for Eve-NG is out of scope for this blog post, there are great guides out there on how to set it up and how the product actually works. I\u0026rsquo;ve just got two tips when deploying this:\nremember to enable MAC Address changes and promiscuous mode on your trunk port groups and prevent yourself from looking like an idiot. make sure your MTUs match. This cost me a significant amount of time, but the Arista vEOS will use MTU 9214 by default on its port groups. If you have a different MTU configured on your physical switch, things will break and you\u0026rsquo;ll be tearing out your hairs in frustration why it\u0026rsquo;s not working. Make sure your MTU matches end-to-end. Nested virtualised networking will perform like a drunk potato. You\u0026rsquo;re running QEMU on top of vSphere. Your switch has no hardware offload whatsoever. You\u0026rsquo;re most likely going to see 50+ms latency even between two switches running on the same Eve-NG machine. Don\u0026rsquo;t expect this to perform well in any way whatsoever. This is purely for testing purposes. Do not configure BFD with this setup, shit will break in unexpected and hilarious ways involving explosions and women and children manning the lifeboats. #NSX-T deployment Now that we have our physical and our nested network set up, i\u0026rsquo;d like to apologise for the lengthy introduction. However, I do believe that it is a required part to get all the following steps , since so much of the NSX-T configuration depends on it, and the amount of nesting involved can cause one to very easily make mistakes. However, now that we\u0026rsquo;re done with this we can finally get to the interesting bit: deploying NSX-T!\nThe topology we\u0026rsquo;re going to be building is as follows:\n##Preparation\nStart off by dowloading all the required bits from your software source, either the nicira portal or the VMware partner portal. We really need all the bits, as there\u0026rsquo;s no automatic deployment of any component from the NSX Manager, as you\u0026rsquo;d expect if you are familiar with NSX-v. Once you\u0026rsquo;re done you should have the following files:\nnsx-manager-1.1.0.0.0.4788147.ova nsx-controller-1.1.0.0.0.4788146.ova nsx-edge-1.1.0.0.0.4788148.ova nsx-edge-1.1.0.0.0.4788148.iso nsx-lcp-1.1.0.0.0.4788198-esx65.zip All components can be directly deployed on the physical ESXi host outside of your nested lab, there is no requirement to run any components on hosts being managed by NSX itself, as it was for NSX-v. For simplicity\u0026rsquo;s sake, i recommend to not deploy any kind of components on the nested hosts to prevent increasing the yodawg-level even more.\nIn addition, make sure you\u0026rsquo;ve reserved static IP addresses for all components, and as a precaution, i\u0026rsquo;ve also created A and PTR records for all components outside of the VTEP interfaces.\n##Deploying the NSX-T Manager and controller.\nIn this step, we\u0026rsquo;ll prepare our deployment by doing the following:\nDeploy the NSX manager. Deploy the NSX controllers. Join the controllers to our management plane. First, we deploy the NSX Manager. This can be done in the regular way by deploying an OVF. All the extended properties are relatively straightforward. I would recommend enabling SSH as it makes configuration significantly simpler.\nNext, deploy at least one controller. Again, this is an OVF deployment without any advanced configuration properties, and enabling SSH significantly increases your quality of life.\nOnce the controller is deployed, log in to both the manager and the controller through either the console or through ssh and run the following commands.\nNote that if you are using putty, for some reason both the NSX manager and Controller will disconnect the session unexpectedly. I have no idea what\u0026rsquo;s causing this, but i\u0026rsquo;ve used bash on windows for this lab.\nOn the manager:\nNSX-Manager\u0026gt; get certificate api thumbprint 973eacda1ec9ff72139dnotgettingthisonesorrya4ef7b3beb4ab6a47bc19287 Note: store this thumbprint for later, you\u0026rsquo;ll need it quite a few times.\nOn the controller:\njoin management-plane ip.of.nsx.mgr username admin thumbprint nsx.mgr.thumbprint Enter the password for your nsx manager, wait for a minute, and ensure that your manager is connected by running the following.\nOn the controller:\nnsxtc\u0026gt; get managers - 172.22.100.22 Connected nsxtc\u0026gt; set control-cluster security-model shared-secret Secret: enteryoursecrethere nsxtc\u0026gt; initialize control-cluster nsxtc\u0026gt; get control-cluster status is master: true in majority: true uuid address status dc59f305-354f-4cdd-bf4b-f4e7f424a2b2 172.22.100.23 active Ensure that is master and in majority are true, and your controller is listed.\nNext, on the manager:\nnsxtm\u0026gt; get management-cluster status Number of nodes in management cluster: 1 - 172.22.100.22 (UUID 4221BA38-FC25-C278-C4D2-B003603F760D) Online Management cluster status: STABLE Number of nodes in control cluster: 1 - 172.22.100.23 (UUID dc59f305-354f-4cdd-bf4b-f4e7f424a2b2) Control cluster status: STABLE nsxtm\u0026gt; get management-cluster status Number of nodes in management cluster: 1 - 172.22.100.22 (UUID 4221BA38-FC25-C278-C4D2-B003603F760D) Online Management cluster status: STABLE Number of nodes in control cluster: 1 - 172.22.100.23 (UUID dc59f305-354f-4cdd-bf4b-f4e7f424a2b2) Control cluster status: STABLE Ensure that all status show as stable and your controller is listed. If you wish to join more controllers, deploy them as the first one and run the following\non each new controller:\nnsxtc\u0026gt; set control-cluster security-model shared-secret Secret: enteryouroriginalsecrethere nsxtc\u0026gt; get control-cluster certificate thumbprint 8caf5b9721446f98dc5dnotgettingthisoneeitherc9702d2fc4d6a469b42499 Then, on the master controller run the following for each of the new controllers:\nnsxtc\u0026gt; join control-cluster ip.of.new.controller thumbprint new.controller.thumbprint Then, on each new controller run the following:\nnsxtc\u0026gt; activate control-cluster Prepare and configure hosts # Once this is all done we can log in to the nsx manager UI. Open a browser and point it towards whatever you configured as the NSX manager\u0026rsquo;s hostname and you\u0026rsquo;ll be presented with the most glorious UI a piece of networking kit has had in a long while. It\u0026rsquo;s clean, it\u0026rsquo;s responsive, it slices, it dices, it makes julienne fries!.\nThe best reason to install NSX-T? No more flash UI. Next, open the fabric tab. Your hosts should be empty, so let\u0026rsquo;s change that. Click the \u0026ldquo;add\u0026rdquo; button, enter a name for your host, its management IP address, your operating system (ESXi) and the username and password. Leave the thumbprint empty as this will automatically be resolved. After adding the host, wait a few minutes until everything is green and the software version shows up correctly. Note that if everything is not green and you\u0026rsquo;re running ESXi 6.5 nested on ESXi6.5, ensure that you\u0026rsquo;ve read /nsx-t-esxi-host-preparation-fails-errno-1-operation-not-permitted-it-is-not-safe-to-continue/ and have disabled secure boot for now.\n##Prepare transport zones and transport nodes.\nNext up, we\u0026rsquo;ll prepare the transport zones. For those coming from NSX-v, the concept of transport zones is slightly different. A transport zone is no longer purely an overlay concept, but dictates which networks can be connected to from a hypervisor or an edge cluster. As such, transport zones come in two forms: Overlay or VLAN transport zones.\nAn Overlay transport zone is what ties the transport nodes together and transports the actual virtual wires. a VLAN transport zone is used for your Tier-0 edges to connect the virtualised network to the physical network.\nFirst off, start with creating an overlay transport zone. Open the Fabric -\u0026gt; Transport zones menu and click add.\nEnter a name for your transport zone, an optional description and a host switch name. The host switch name will be used to create a mapping between physical nics on your ESXi hosts and either logical router nics or VTEP interfaces.\nNext, create at least one IP pool for your VTEP interfaces. Open Inventory -\u0026gt; Groups -\u0026gt; IP Pools and create at least one IP pool. As i\u0026rsquo;ve created two different \u0026ldquo;sites\u0026rdquo; in my lab, i\u0026rsquo;ve created two IP pools using distinct IP subnets. This part is very similar to NSX-v\u0026rsquo;s IP pool creation: enter a name, add an IP range, gateway, subnet and optional DNS servers.\nWhen the IP pools are created, open Fabric -\u0026gt; Profiles and create a new Uplink Profile. The uplink profile determines the VTEP interface configuration. While you can use the default, it is recommended to create your own. I\u0026rsquo;ve provided an over overview of the uplink profiles i\u0026rsquo;ve created to serve as an example, but right now you only need one or two (depending on whether you\u0026rsquo;re using separate L3 domains for your ESXi hosts:\nEnter a name and an optional description. If you want LAGs, configure a lag (which has its use cases for physical edges, but that\u0026rsquo;s something we\u0026rsquo;re not going into right now). Set a Teaming policy, and depending on your teaming policy set active and standby uplinks as uplink-1 and uplink-2.\nNext, set a transport VLAN which will be your VTEP vlan, and set the MTU to a minimum of 1600.\nOnce the Uplink profiles are created, add your ESXi nodes as a transport node. A transport node defines a node as participating in your network fabric. Open Fabric -\u0026gt; Nodes -\u0026gt; Transport Nodes and click \u0026ldquo;Add\u0026rdquo;.\nEnter a name (recommended to use the name of your host), select your ESXi host from the dropdown, and select the Overlay transport zone you\u0026rsquo;ve just created. Then, in the new node switch enter your host switch name as the host switch name you\u0026rsquo;ve configured in the transport zone, and in the Uplink profile enter a name for your VTEP. For the Uplink profile, select your Uplink profile you\u0026rsquo;ve created, and for IP assignment select either \u0026ldquo;Use IP Pool\u0026rdquo;, \u0026ldquo;Use DHCP\u0026rdquo; or \u0026ldquo;Use Static IP List\u0026rdquo;. The first two should speak for itself if you have NSX-v experience, the third one is new and allows you to configure a static IP directly on a host.\nIf you are using DHCP ensure you have your DHCP server set up, if you are using IP pools click the \u0026ldquo;Create and use new IP pool\u0026rdquo; and enter a name and a subnet in the same way you\u0026rsquo;d do this in NSX-v.\nNext, in the physical NICs section, select a physical device and assign it an uplink from your uplink profile. Remember when i said to leave two nics unused in your nested ESXi host? This is the reason why, as NSX-T needs dedicated physical NICs for its VTEP interfaces.\nIn our case we add vmnic3 and vmnic4 to be our uplink-1 and uplink-2 respectively. Note that there is a weird UI bug when adding and selecting a second vmnic from the dropdown, but you can enter the name manually.\nAfter waiting a short while the transport node should show up and its status should be \u0026ldquo;Success\u0026rdquo;. If you log in to vSphere and open the physical adapter configuration of your ESXi host it should show the physical nic is assigned to your overlay switch, even though the switch itself does not appear anywhere within vSphere. I believe this is a quality of life issue and it would be nice to show the switch itself as an opaque switch, but for now we can use this to validate the host is actually connected to the NSX-T opaque host switch.\nNow that we have prepared our cluster, let\u0026rsquo;s deploy some edge hosts!\n##Edge host deployment and configuration\nBefore we start with the actual edge host configuration, there are some things to consider if you\u0026rsquo;re coming from NSX-v. Whereas an Edge is a virtual appliance performing actual routing, natting, and advanced services inside the VM, the concept of an edge is slightly different in NSX-T.\nAn edge is either a virtual or physical form factor server, which serves as the control plane for routing and advanced services in NSX-T. Whereas in NSX-v one would deploy an edge per router, NSX-T virtualises these resurces as containers running inside an edge node, also named VRFs. So instead of having multiple edges, a NSX-T environment will only have a limited amount of edges which then create resources such as Tier-0 and Tier-1 routers, DHCP servers and NAT services on-demand. Not only does this significantly reduce VM sprawl, it also simplifies configuration, allows for rapid provisioning and reduces complexity of the environment.\nEdges can be deployed in multiple ways (OVA deployment, PXEBoot, ISO installation. In this deployment, we\u0026rsquo;ll focus on the OVA deployment, but from a technical point of view they are all equal.\nStart off with deploying the edge through the OVA deployment. During the deployment one can select the size, which determines the throughput and performance of advanced services. For this deployment we\u0026rsquo;ll select small as it is only a Poc and our resources are limited.\nWhen it comes to the selection of network ports, you have to follow a specific setup. While this can be changed afterwards, it is recommended to do it correct at deployment time:\nNetwork 0 should be connected to your management VLAN port group as this will be used to configure the edge and log in to it over SSH. Network 1 should be connected to either your trunk or a dedicated VTEP port group. Network 2 and 3 should be connected to either individiual /31 transit VLAN port groups or a trunk containing your uplinks. This design choice is entirely dependent on your Tier-0 configuration, for this example we\u0026rsquo;ll use dedicated port groups for transit purposes. Just as with NSX-v it is recommended to set these transit port groups to active/unused and unused/active and have the actual routing protocol handle redundancy. Fortunately we\u0026rsquo;ve already preprovisioned these transit VLANs for these kind of use cases, if not you\u0026rsquo;ll have to build them yourself.\nOnce the edges are deployed (you have deployed two, didn\u0026rsquo;t you?), log in to the edge over SSH if you\u0026rsquo;ve enabled that during the deployment, and run the following\nnsxte-0\u0026gt; join management-plane ip.of.nsx.mgr username admin thumbprint nsxt.mgr.thumbprint if you don\u0026rsquo;t have the thumbprint anymore, you can retrieve it from the manager with the command get certificate api thumbprint.\nYou should get a message stating that the edge succesfully joined the NSX fabric as a fabric node with its UUID. repeat this step for each edge node.\nWhen you log back in to the NSX manager UI, you should see your edge nodes under Fabric -\u0026gt; Nodes -\u0026gt; Edges. Note that LCP connectivity is not available, this is entirely normal until the host is joined as a transport node. Next up, we\u0026rsquo;ll prepare the edge as an actual transport node and join it to the transport zones.\nFirst off, we\u0026rsquo;ll need to create transport zones for your VLAN uplinks. Remember i told you how transport zones are different from NSX-v? This is where that comes in. When we created a transport zone for logical switching earlier, we selected the type as Overlay. Now, we\u0026rsquo;ll go to Fabric -\u0026gt; Transport zone and create 4 transport zones, one for each of our transit VLANs.\nAfter the transport zones have been created, add your Edge node as a transport node to join it to the fabric. Open Fabric -\u0026gt; Nodes -\u0026gt; Transport Nodes and add a new transport node.\nThis procedure is the same as for an ESXi host with the difference that this edge node will be connected to multiple overlays. If you consider your typical NSX-v design, you\u0026rsquo;d have port groups for transit VLANs and a VTEP vlan on your vSphere edge cluster, and this is very similar.\nEnter a name and select your node, then in the transport zones field select your overlay and two VLAN transport zones. Note that you can add as much transport zones to an edge transport node as you wish, but for our use case we\u0026rsquo;re using two VLAN transport zones and a single overlay zone.\nNext, we add the Node switches. This is very similar to the ESXi transport node configuration, however we need to create multiple node switches, one for each transport zone. When you\u0026rsquo;ve configured the first node switch, click \u0026ldquo;Add new node switch\u0026rdquo; at the bottom to create new ones for each transport zone. Again, the edge switch names must match what you\u0026rsquo;ve configured in the transport zone. When creating a VLAN backed node switch, you cannot configure an IP profile, because a VLAN transport zone does not require VTEP interfaces.\nAlso note that for virtual edge transport nodes, only a single virtual nic can be bound to a tz overlay. For virtual edges, this is something that would be resolved through your distributed switch to which the virtual appliance is connected. For physical edges you can use active/standby uplinks as normal. This is also the reason we\u0026rsquo;ve created separate uplink profiles for our edges.\nWhen you\u0026rsquo;re done, the edge transport node should look something like this:\nWhen the status of the edges is all OK (check for the LCP status in the edge panel, note that this may take a few minutes), start creating an edge cluster. Open the Fabric -\u0026gt; Nodes -\u0026gt; Edge clusters panel and add a new edge cluster. Note that this is mandatory, even if you only have a single edge transport node.\nEnter the name, an optional description and select the standard edge cluster profile. Next, click edit next to the transport nodes, select \u0026ldquo;Virtual machine\u0026rdquo; and select both edges. If you\u0026rsquo;ve deployed edges as a physical machine (which can still be virtual but needs to be deployed through the ISO or PXEBoot), they\u0026rsquo;ll appear as physical machines.\nOnce the edge cluster is created, we can actually start creating logical routers. Open the Routing panel, then add a new router and select \u0026ldquo;New Tier-0 router\u0026rdquo;. Tier-0 routers are what connect your physical network to your virtualised network fabric. Unlike the NSX Edge service gateway however, the Tier-0 router is fully distributed and purely functions as a physical to virtual perimeter router. As such, it does not support the features that a Tier-1 router does.\nEnter a name and an optional description, select your edge cluster and select your high availability mode. This choice is up to you, active-standby is comparable to NSX-v\u0026rsquo;s HA deployment (but without the long failover times), whereas active-active will configure ECMP to your physical routers. As we like to explore ECMP, we\u0026rsquo;ll select active-active.\nWhen deploying the logical router, you\u0026rsquo;ll notice the speed at which it deploys. As there are no appliances or virtual machines to deploy or boot, new routing instances can literally be configured at the speed at which a container can be spun up, which will make automated deployments significantly easier.\nNext, create a Tier-1 router. Tier-1 routers are what connects your logical switches to your tier-0 routers and provide advanced services such as DHCP and NAT, while still being fully distributed. They can be compared as a hybrid between NSX-v\u0026rsquo;s distributed logical router and edge service gateway, with all routing being performed in-kernel while stateful services have their control plane on the edge service router which exists on the edge transport node.\nWhen creating a Tier-1 router enter a name and optional description and select your upstream Tier-0 Router. This automatically creates internal logical switches between the Tier-0 and Tier-1 router. Next, select your edge cluster, edge cluster members and preferred member. Due to the way Tier-1 routers work, they cannot be configured as active-active and we\u0026rsquo;ll need to select a primary member on which services run.\nWhen both Tier-0 and Tier-1 routers have been created, your router should look something like this.\nConfigure logical routing and uplinks # Next to create connectivity, we\u0026rsquo;ll create logical switches. This is very comparable to NSX-v, with one major difference: VLAN backed port groups are also logical switches, as opposed to port groups in vSphere.\nOpen the switching panel and start adding logical switches. For this demo we\u0026rsquo;ve created the following logical switches:\nLS1 LS2 vlan-1012 vlan-1014 vlan-1016 vlan-1018 The LS logical switches are created in the tz-overlay transport zone, and the vlan logical switches are created in their respective vlan transport zone. The VLAN logical switches have been created with VLAN tag 0, as these are connected to a port group configured as an access port. If you were to connect these to a trunk port a VLAN tag msut be configured.\nNote that for the logical switches you must select a Replication mode, either Hierarchical Two-Tier or Head. Hierarchical Two-Tier is very comparable to NSX-v\u0026rsquo;s Hybrid replication mode where a UTEP is elected in each L2 broadcast domain which replicates trafic to other broadcast domains. Head is very similar to unicast replication mode. Note that there is no pure multicast option anymore. If you want to, switching profiles can be configured which allow for advanced security, monitoring and traffic shaping options such as DSCP/CoS, Spoofguard, ARP and DHCP snooping, etc.\nWhen configured, your logical switches should look comparable to this.\nNext, go back to your router overview. Click the Tier-1 Router name and select configuration. Add a Logical Router port, enter a name, select your logical switch to connect the port to and configure an IP address and subnet mask.\nIf you open the switching -\u0026gt; Ports panel, you should see your logical routers being connected to your logical switch. When you click the attachment you can see the details about the logical router\u0026rsquo;s port. Note that in this example i\u0026rsquo;ve also created DHCP services on the logical switches, but this is not required and left as an exercise for the reader.\nNow that your Tier-1 router has been configured, we can test basic connectivity. Create virtual machines on your nested ESXi hosts, and when connecting their virtual NICs to a network, you should see two new port groups with a unique interface. This is an opaque switch and is unique to NSX-T. It cannot be managed or configured from vSphere, is unique to each host and is completely independent of vCenter.\nConfirm that - once an IP address has been configured on the virtual machines - that you can ping the Tier-1 logical router IP address, and the VM on the other logical switch.\nNow that we\u0026rsquo;ve configured connectivity between logical switches, it\u0026rsquo;s time to configure actual physical connectivity. Open the Routing panel, and select the Tier-0 router we\u0026rsquo;ve configured earlier. Add a new router port, enter a name and optional description and select the uplink type. Next, select the first transport node and the first transit VLAN logical switch. The configuration should look something like this.\nRepeat these steps for each transit logical switch, and ensure the correct logical switches are connect to the correct transport mode.\nWhen finished, the result should be one downlink port to your Tier-1 router and four uplink ports to your VLAN logical switches:\n##Configure upstream routing\nNext, enter the Routing panel in the Tier-0 router and select BGP. First, edit the global BGP configuration. In our case we\u0026rsquo;ll set BGP to enabled, enable ECMP, leave graceful restart disabled and set our local AS to 65538. Next, add a new Neighbour.\nEnter the following information:\nNeighbour address: the IP address of your BGP neighbor. Local Address: select only the transit VLAN your BGP neighbour is connected to. Remote AS: Enter the remote AS of your neighbor. Keep Alive Time: This is dependent on the equipment you are peering with. Hold Down time: This is dependent on the equipment you are peering with.\nWhen all of this has been configured, we can log in to either of the Tier-0 router appliances and run the following commands\nget logical-router look for the SERVICE_ROUTER_TIER_0 and note down its VRF. This is the routing engine responsible for BGP peering. Next, run vrf vrfid - where vrfid is the VRF ID of the Tier-0 service router - to enter the VRF.\nNext, run get route bgp to confirm that we are receiving routes from our upstream physical devices, and we are in fact getting ECMP enabled routes\nnsxte-0(tier0_sr)\u0026gt; get route bgp Flags: c - connected, s - static, b - BGP, ns - nsx_static nc - nsx_connected, rl - router_link, t0n: Tier0-NAT, t1n: Tier1-NAT b 0.0.0.0/0 [20/0] via 172.20.255.12 b 0.0.0.0/0 [20/0] via 172.20.255.14 b 172.16.0.0/12 [20/0] via 172.20.255.12 b 172.16.0.0/12 [20/0] via 172.20.255.14 b 192.168.0.0/16 [200/0] via 0.0.0.0 b 192.168.1.0/24 [20/0] via 172.20.255.12 b 192.168.1.0/24 [20/0] via 172.20.255.14 b 217.63.252.0/23 [20/0] via 172.20.255.12 b 217.63.252.0/23 [20/0] via 172.20.255.14 Note that we are also getting 192.168/24 routes, these are in fact OSPF routes for another lab redistributed by NSX-v into BGP by my physical router.\nThe next step is to configure route redistribution from our Tier-1 to our Tier-0. One important thing to note is that these routes are distributed through the NSX-T control plane and not through a routing protocol.\nOpen the Tier-1 router, click routing and select \u0026ldquo;Route advertisement\u0026rdquo;. First, configure the global options. We\u0026rsquo;ll enable route advertisement and advertise all NSX connected routes. We won\u0026rsquo;t advertise NAT routes or Static routes since we\u0026rsquo;re not using those in our lab.\nWhen we run get route on our Tier-0 service router VRF, we can see that two additional routes have been injected into the routing table.\nnsxte-0(tier0_sr)\u0026gt; get route Flags: c - connected, s - static, b - BGP, ns - nsx_static nc - nsx_connected, rl - router_link, t0n: Tier0-NAT, t1n: Tier1-NAT Total number of routes: 11 b 0.0.0.0/0 [20/0] via 172.20.255.12 b 0.0.0.0/0 [20/0] via 172.20.255.14 ns 10.1.0.0/24 [3/0] via 169.254.0.1 ns 10.2.0.0/24 [3/0] via 169.254.0.1 rl 100.64.0.0/31 [0/0] via 169.254.0.1 c 169.254.0.0/28 [0/0] via 169.254.0.2 b 172.16.0.0/12 [20/0] via 172.20.255.12 b 172.16.0.0/12 [20/0] via 172.20.255.14 c 172.20.255.12/31 [0/0] via 172.20.255.13 c 172.20.255.14/31 [0/0] via 172.20.255.15 b 192.168.0.0/16 [200/0] via 0.0.0.0 b 192.168.1.0/24 [20/0] via 172.20.255.12 b 192.168.1.0/24 [20/0] via 172.20.255.14 b 217.63.252.0/23 [20/0] via 172.20.255.12 b 217.63.252.0/23 [20/0] via 172.20.255.14 The ns routes, which is a special type for NSX routing have been propagated by the Tier-1 logical router into the Tier-0 logical router routing table.\nThe last step we need to perform is to redistribute our routes into the physical world. Open the Tier-0 Router, selecting Routing and open \u0026ldquo;Route redistribution\u0026rdquo;. First, ensure that route redistribution is enabled. Next, add a new route redistribution. Enter a name, an optional description and select your sources. In our case we\u0026rsquo;re only selecting \u0026ldquo;NSX Static\u0026rdquo; as we have no need to distribute natted or manual static routes. Note that NSX static routes are auto-configured routes redistributed into the fabric, whereas static routes are manually configured static routes on the Tier-1 router. This can cause some confusion and initially cost me a good 30 minute to discover why route redistribution was not working..\nAs we can see on the arista switches we\u0026rsquo;re receiving our 10.1.0.0/24 and 10.2.0.0/24 routes:\nlocalhost#show ip route bgp VRF name: default Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1, E2 - OSPF external type 2, N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type2, B I - iBGP, B E - eBGP, R - RIP, I L1 - ISIS level 1, I L2 - ISIS level 2, O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service Gateway of last resort: B E 0.0.0.0/0 [200/0] via 172.20.255.8, Vlan1008 B E 10.1.0.0/24 [200/0] via 172.20.255.17, Vlan1016 B E 10.2.0.0/24 [200/0] via 172.20.255.17, Vlan1016 B E 172.20.255.0/31 [200/0] via 172.20.255.8, Vlan1008 B E 172.20.255.2/31 [200/0] via 172.20.255.8, Vlan1008 B E 172.20.255.4/31 [200/0] via 172.20.255.8, Vlan1008 B E 172.20.255.6/31 [200/0] via 172.20.255.8, Vlan1008 B E 172.20.255.10/31 [200/0] via 172.20.255.8, Vlan1008 B E 172.20.0.0/16 [200/0] via 172.20.255.8, Vlan1008 B E 172.21.100.0/24 [200/0] via 172.20.255.8, Vlan1008 B E 172.21.101.0/24 [200/0] via 172.20.255.8, Vlan1008 B E 172.21.102.0/24 [200/0] via 172.20.255.8, Vlan1008 B E 172.22.0.0/16 [200/0] via 172.20.255.8, Vlan1008 B E 172.23.0.0/16 [200/0] via 172.20.255.8, Vlan1008 B E 172.24.0.0/16 [200/0] via 172.20.255.8, Vlan1008 B E 192.168.1.0/24 [200/0] via 172.20.255.8, Vlan1008 B E 192.168.0.0/16 [200/0] via 172.20.255.19, Vlan1018 B E 217.63.252.0/23 [200/0] via 172.20.255.8, Vlan1008 And that\u0026rsquo;s it! To prove that we\u0026rsquo;ve got connectivity i\u0026rsquo;ve performed a HTTP GET to my own blog, which shows that we can connect to an external network!\nHopefully this has helped you in understanding how NSX-T works, the differences between NSX-v and NSX-T and how you too can build your own multi-site NSX-T lab with relatively limited resources by using nested networking equipment and nested hypervisors.\nConfigurations # For completeness, the arista vEos configurations have been provided below:\nvEOS 1\n! Command: show running-config ! device: localhost (vEOS, EOS-4.18.1F) ! ! boot system flash:/vEOS-lab.swi ! transceiver qsfp default-mode 4x10G ! spanning-tree mode mstp ! no aaa root ! username admin role network-admin secret sha512 redacted ! vlan 1008,1016,1018,2200,2203 ! interface Ethernet1 no switchport ip address 172.31.255.1/31 ! interface Ethernet2 mtu 9214 switchport access vlan 2200 switchport mode trunk ! interface Ethernet3 ! interface Management1 ! interface Vlan1008 ip address 172.20.255.9/31 ! interface Vlan1016 ip address 172.20.255.16/31 ! interface Vlan1018 ip address 172.20.255.18/31 ! interface Vlan2000 ! interface Vlan2200 mtu 9214 ip address 172.22.100.135/24 ! interface Vlan2203 ip address 172.22.103.145/24 ! ip routing ! router bgp 65536 neighbor pg-nsxt-1 peer-group neighbor pg-nsxt-1 remote-as 65538 neighbor pg-nsxt-1 maximum-routes 12000 neighbor 172.20.255.8 remote-as 65535 neighbor 172.20.255.8 maximum-routes 12000 neighbor 172.20.255.17 peer-group pg-nsxt-1 neighbor 172.20.255.19 peer-group pg-nsxt-1 aggregate-address 172.16.0.0/12 summary-only ! end vEOS2\nlocalhost(config-router-bgp)#show running-config ! Command: show running-config ! device: localhost (vEOS, EOS-4.18.1F) ! ! boot system flash:/vEOS-lab.swi ! transceiver qsfp default-mode 4x10G ! spanning-tree mode mstp ! no aaa root ! username admin role network-admin secret sha512 redacted ! vlan 1010,1012,1014,2200,2203 ! interface Ethernet1 no switchport ip address 172.31.255.0/31 ! interface Ethernet2 switchport access vlan 2200 switchport mode trunk ! interface Ethernet3 ! interface Management1 ! interface Vlan1010 ip address 172.20.255.11/31 ! interface Vlan1012 ip address 172.20.255.12/31 ! interface Vlan1014 ip address 172.20.255.14/31 ! interface Vlan2200 mtu 9214 ip address 172.22.100.136/24 ! interface Vlan2203 ip address 172.22.103.146/24 ! ip routing ! router bgp 65537 neighbor pg-nsxt-1 peer-group neighbor pg-nsxt-1 remote-as 65538 neighbor pg-nsxt-1 maximum-routes 12000 neighbor 172.20.255.10 remote-as 65535 neighbor 172.20.255.10 maximum-routes 12000 neighbor 172.20.255.13 peer-group pg-nsxt-1 neighbor 172.20.255.15 peer-group pg-nsxt-1 aggregate-address 172.16.0.0/12 summary-only ! end ","date":"29 April 2017","permalink":"/posts/building-a-nsx-t-nested-lab/","section":"Posts","summary":"Since my last tweet and blogpost on a bug in NSX-T when deploying on a nested ESXi host , i\u0026rsquo;ve had a few requests from people to describe the actual lab setup used, the procedure and a quick-and-dirty guide to get started with NSX-T on vSphere.","title":"Building a NSX-T nested lab with Eve-NG, virtualised switches, BGP, and the kitchen sink attached."},{"content":"As today was a lab day for the first time in a long while, i decided to finally get around to finishing my NSX-T nested lab.\nSo after deploying the manager and controller cluster, the time comes to prepare the ESXi hosts. And as we are preparing we run into the following error: Which in itself is not very helpful unfortunately. So off to the ESX cli we go.\nFirst, we try to manually install the software itself:\nesxcli software vib install -d /vmfs/volumes/T2\\ -\\ SATA/nsx-lcp-1.1.0.0.0.4788198-esx65.zip Which fails with the error\nCould not install image profile: ([], \u0026ldquo;Error running command \u0026lsquo;[\u0026rsquo;/etc/init.d/nsx-da\u0026rsquo;, \u0026lsquo;start\u0026rsquo;, \u0026lsquo;install\u0026rsquo;]\u0026rsquo;: [Errno 1] Operation not permitted\\nIt is not safe to continue. Please reboot the host immediately to discard the unfinished update.\u0026rdquo;)\nEvery time the service seems to be a different one, but the general error seems to be the same. When we search for \u0026ldquo;nsx\u0026rdquo; in the logs, we end up with a variety of messages, but a significant amount of warnings appear related to secure boot. An example is\nVisorFSTar: 1484: File (usr/lib/vmware/nsx-da/setDaUser.py) has both the sticky bit and exec bit set. Incompatible with secure boot As i\u0026rsquo;ve recently rebuilt my lab, i\u0026rsquo;ve done this with 6.5. My nested lab used for NSX-T is a 6.5 lab as well, and NSX 6.5 supports secure boot, the nested ESXi instance will be configured and running with secure boot if your VM was configured with EFI (which seems to be the default if you select VMware ESXi6.5 as a guest operating system for your VM.\nAfter we power off the nested ESXi hosts and disable Secure Boot, the installation works just fine. So if you\u0026rsquo;re running into this issue, disabling secure boot should allow you to install the NSX-T kernel modules just fine. If required you should be able to reenable it after preparing the hosts.\nSecure boot can be disabled on a per-VM basis in VM-\u0026gt;Edit Settings-\u0026gt;VM Options-\u0026gt;Boot Options-\u0026gt;Secure Boot. Note that your VM must be powered off to change this.\n","date":"21 April 2017","permalink":"/posts/nsx-t-esxi-host-preparation-fails-errno-1-operation-not-permitted-it-is-not-safe-to-continue/","section":"Posts","summary":"As today was a lab day for the first time in a long while, i decided to finally get around to finishing my NSX-T nested lab.\nSo after deploying the manager and controller cluster, the time comes to prepare the ESXi hosts.","title":"NSX-T Nested ESXi host preparation fails."},{"content":"For a project i\u0026rsquo;m currently working on, we need to provide a full end-to-end validation of the NSX routing, distributed firewall rules and VXLAN functionality. As the amount and complexity of firewall rules are quite significant, i\u0026rsquo;ve written a script that allows you to run and retrieve data from NSX Traceflow in an automated fashion, which can be found at https://bitbucket.org/srobroek/pnsx-traceflow/ . (Note: this is still very preliminary, needs some feature updates and hopefully will at some point be merged into PowerNSX).\nIn addition i\u0026rsquo;d like to warn that this is not the lightest reading material. It presumes the reader has sufficient knowledge in NSX, the distributed firewall, some PowerShell and API usage. It doesn\u0026rsquo;t contain any pictures, but it does contain some amazing code ;)\nThe functionality of the script is relatively simple: One can call the start-NSXTraceFlow command with a variety of options. All of the options can be found in the sourcecode or by running the cmdlet, in this example we\u0026rsquo;re using the following ones:\nSourcevNic - This is mandatory as the NSX API requires a vNic connected to a VXLAN as a source. Currently no check is performed if the vNic is connected to a VXLAN, but this should be relatively simple and can be considered a future improvement. Protocol - This determines the protocol and determines dynamic parameters related to the specific protocol. TrafficType - This determines l2, l3, unicast, multicast or broadcast. Currently only unicast is guaranteed to be working, multicast should work and broadcast needs to be implemented. Destination - This can either be a DestinationvNic, DestinationIP or DestinationMac, depending on the TrafficType. SourcePort - The source port for tcp or udp protocols DestinationPort - The destination port for tcp or udp protocols. As an example, the way i\u0026rsquo;ve used this in my lab is as follows:\nStart-NSXTraceflow -SourcevNic $nic1 -Protocol tcp -TrafficType l2-unicast -destinationvNic $nic2 -SourcePort 12345 -destinationPort 443. When ran, this provides an object in the following format:\nTraceFlowId ----------- 00000000-0000-0000-0000-00000e3ebc30 Which in turn can be utilised by either Get-NSXTraceFlowResult, providing the following resulting object:\nvnicId : 502147c0-ad14-6de3-7abc-ef2250633df8.000 id : 00000000-0000-0000-0000-00000e3ebc30 receivedCount : 1 forwardedCount : 0 deliveredCount : 1 logicalReceivedCount : 2 logicalDroppedCount : 0 logicalForwardedCount : 2 timeout : 10000 completeAvailable : true result : SUCCESS resultSummary : Traceflow delivered observation(s) reported srcIp : 172.20.205.10 srcMac : 00:50:56:a1:49:79 dstMac : 172.20.205.11 Or Get-NSXTraceFlowObservations, which provides a much more extensive object, of which i\u0026rsquo;m showing a sample of the data below:\nThe main object # pagingInfo : pagingInfo traceflowObservationReceived : traceflowObservationReceived traceflowObservationLogicalReceived : {traceflowObservationLogicalReceived, traceflowObservationLogicalReceived} traceflowObservationLogicalForwarded : {traceflowObservationLogicalForwarded, traceflowObservationLogicalForwarded} traceflowObservationDelivered : traceflowObservationDelivered The logical forwarding overview of the traceflow # roundId : 00000000-0000-0000-0000-00000e3ebc30 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 2 ruleId : 1006 roundId : 00000000-0000-0000-0000-00000e3ebc30 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 4 ruleId : 1006 As you can see above, this is in a single host in my lab, without any kind of routing. The traceflow hits the default firewall rule as it exits the source VM, and hits the default firewall rule again as it reaches the destination VM, so it\u0026rsquo;s not very exciting.\nLet\u0026rsquo;s show the same result for two VMs on a different VXLAN with two different types of firewall rules scoped for both individual VMs, one deny and one allow rule:\nFirst off, we start with a connection on port 443, which is allowed:\nStart-NSXTraceflow -SourcevNic $nic1 -Protocol tcp -TrafficType l3-unicast -destinationvNic $nic2 -SourcePort 12345 -destinationPort 443 Now, when we look at the traceflowObservatioLogicalFowarded Object in our results we see the following:\nroundId : 00000000-0000-0000-0000-00007ef51715 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 2 ruleId : 1021 roundId : 00000000-0000-0000-0000-00007ef51715 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : LS compDisplayName : LB hopCount : 3 vni : 10006 logicalCompId : universalwire-22 logicalCompName : LB roundId : 00000000-0000-0000-0000-00007ef51715 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : LR compDisplayName : udlr1 hopCount : 5 vni : 10004 lifName : 27100000000c compId : 10000 compName : default+edge-0503a72f-955c-4a96-85f6-20b1306c24fc srcNsxManager : 422185e4-b4ce-aae7-c07e-2fb72e0a19bd srcGlobal : true logicalCompId : edge-0503a72f-955c-4a96-85f6-20b1306c24fc logicalCompName : udlr1 otherLogicalCompId : universalwire-18 otherLogicalCompName : NSXRouted-1-a1190f55-ba2f-4554-a474-57cc2af19d7a roundId : 00000000-0000-0000-0000-00007ef51715 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 8 ruleId : 1021 As you can see, the results contain the exact steps taken through the NSX network. It shows the firewall rule being hit (ID 1021), the traffic being routed through the UDLR called udlr1, and on the receiving side the traffic hitting the same DFW rule again.\nTo validate that this is indeed the rule, we can use get-nsxfirewallrule to retrieve the rule and get various properties, in this case we get the name.\n(Get-NsxFirewallRule |? {$_.id -eq 1021}).Name allow https for traceflow Now we run the same with port 80 tcp as the destination port:\nStart-NSXTraceflow -SourcevNic $nic1 -Protocol tcp -TrafficType l3-unicast -destinationvNic $nic2 -SourcePort 12345 -destinationPort 80 This time, the traceflowresults show that the traffic was not delivered:\nGet-NSXTraceflowResult 00000000-0000-0000-0000-000032bc3c80 operState : COMPLETE vnicId : 5021ddf8-a7f9-7da3-66f6-f17319970ccd.000 id : 00000000-0000-0000-0000-000032bc3c80 receivedCount : 1 forwardedCount : 0 deliveredCount : 0 logicalReceivedCount : 1 logicalDroppedCount : 1 logicalForwardedCount : 0 timeout : 10000 completeAvailable : true result : FAILURE resultSummary : Traceflow dropped observation(s) reported srcIp : 172.20.205.11 srcMac : 00:50:56:a1:99:b2 dstMac : 192.168.10.50 Now, when we look at the traceflow observations we see that the object contains a new object called traceflowObservationLogicalDropped containing the following:\nroundId : 00000000-0000-0000-0000-000032bc3c80 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 2 ruleId : 1022 dropReason : FW_RULE This shows us why it was dropped, at what phase it was dropped (the hopcount can be used for this combined with the other Traceflow results), and the ruleId that dropped it.\nwhen we look at the traceflowObservationLogicalReceived we can see that the last (and in this case, first) step in the flow was the firewall rule shown above. So now we know when in the process it was dropped. For comparison, if we were to apply the rule only to the destination VM, we\u0026rsquo;d see the following observationlogicalreceived:\nroundId : 00000000-0000-0000-0000-00004dc95248 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 1 roundId : 00000000-0000-0000-0000-00004dc95248 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : LR compDisplayName : udlr1 hopCount : 4 vni : 10006 lifName : 27100000000b compId : 10000 srcNsxManager : 422185e4-b4ce-aae7-c07e-2fb72e0a19bd srcGlobal : true compName : default+edge-0503a72f-955c-4a96-85f6-20b1306c24fc logicalCompId : edge-0503a72f-955c-4a96-85f6-20b1306c24fc logicalCompName : udlr1 otherLogicalCompId : universalwire-22 otherLogicalCompName : LB roundId : 00000000-0000-0000-0000-00004dc95248 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : LS compDisplayName : NSXRouted-1-a1190f55-ba2f-4554-a474-57cc2af19d7a hopCount : 6 vni : 10004 logicalCompId : universalwire-18 logicalCompName : NSXRouted-1-a1190f55-ba2f-4554-a474-57cc2af19d7a roundId : 00000000-0000-0000-0000-00004dc95248 transportNodeId : 40e3459c-d04e-4453-89ff-7ae299542555 hostName : esxi.int.vxsan.com hostId : host-29 component : FW compDisplayName : Firewall hopCount : 7 This shows that the traffic is actually allowed out of the source VM, routed through the DLR, but dropped at the same rule when it arrives at the source VM.\nHopefully i\u0026rsquo;ve shown you the power of the NSX API, traceflow and automating this. While this is specifically written in powershell, the API calls are language-independent and could be repurposed for various purposes. Think vRealize Orchestrator, VMware\u0026rsquo;s Houdini product for change validation, testing on-demand firewall rules as a day-2 operation from vRealize Automation, integration with your CI/CD system, or even automated security auditing and policy validation.\nThe next part of this blogpost will be about the actual process how we used the NSX traceflow API to validate a complex set of service composer based policies to prove to the customer that the security policies created in NSX. Since the environment will go live straight from handover, there can be no mistakes in the firewall configuration as any change may take weeks after the go-live date. As such, we\u0026rsquo;re automating all steps to provide a report proving that our firewall rules do what they are expected to do.\nI hope you\u0026rsquo;ll enjoy the script i\u0026rsquo;ve provided, and possibly you might be able to use it in your own environment, and as soon as i find the time i\u0026rsquo;ll provide part two of this series. Until then, happy powershelling!\n","date":"6 April 2017","permalink":"/posts/automated-nsx-dfw-validation-with-powernsx/","section":"Posts","summary":"For a project i\u0026rsquo;m currently working on, we need to provide a full end-to-end validation of the NSX routing, distributed firewall rules and VXLAN functionality. As the amount and complexity of firewall rules are quite significant, i\u0026rsquo;ve written a script that allows you to run and retrieve data from NSX Traceflow in an automated fashion, which can be found at https://bitbucket.","title":"Automated NSX DFW validation with PowerNSX"},{"content":"Full Disclosure: This blogpost is written for and part of the VMware vExpert NSX Certification blogging campaign.\nA long long time ago when i was still making my first steps in the wonderful world of IT, my role was that of the go-to operations guys for small-medium businesses. The main focus of my work at that time was on system administration, primarily around linux, windows and networking.\nSince that time, i\u0026rsquo;ve gone through many roles and worked with a variety of products, but one of the vendors that\u0026rsquo;s always been in the picture has been VMware. I\u0026rsquo;ve started with virtualisation quite a while ago actually, first with Mac-On-Linux and KVM, a VMware GSX and later Server machine running some workloads for a non-profit organisation, all very basic. Later on, i got to touch the earlier releases of the VMware products more and more often as part of my daily job, and relative late during the vSphere 4 release i finally took the plunge and certified myself as a VCP (we didn\u0026rsquo;t have any of those fancy suffixes back then), starting off a long journey of learning something new every day\u0026hellip;\n##Why did you decide to take your first test and what was your motivation?\nTo be completely open and honest, at first did not really care that much about VMware certifications back then. I was a Linux and networking guy, and vSphere was mostly a means to an end. But as i was working with it more and more and getting more enthousiastic, combined with the need to get more VMware certified employees, my certification was actively encouraged and sponsored through my old employer OGD and a colleague ( Joep Piscaer , thanks for getting me involved ;)). As i was also slowly getting involved in more projects as opposed to operations, i noticed my virtualisation skills were still lacking when it came to more than just operations, and as such, i was sent off to the VCP 4 ICM training.\n##What was your journey for the first test?\nMost of my experience with vSphere came from just hands-on experience. I had zero knowledge on how to actually design or implement the product, but by trying something new every day you slowly get more and more familiar with the product and comfortable performing changes, upgrades and even suggesting improvements to the current implementation.\nAs well as the hands-on experience, as i wasn\u0026rsquo;t certified i still had to take the mandatory ICM training. While not all subjects of the course were on a VCP-level, it did teach me a lot about all the features that you\u0026rsquo;re usually not touching during normal operations.\n##Were you nervous, how did you study?\nObviously, since a full week of ICM training is not cheap, i was slightly nervous. The VMware VCP exams are not known as being extremely easy, and back then my certification-fu was not as strong as it is today. Being only familiar with Microsoft, LPIC and Cisco certifications, VMware\u0026rsquo;s examination style was completely new for me. My typical study plan was to get as much hands-on experience during the ICM as possible and go through the course material at home. Since i didn\u0026rsquo;t have a homelab back then i was dualbooting my desktop at home, running ESX as a virtual machine on my employer\u0026rsquo;s desktop, and occasionally in my old employer\u0026rsquo;s lab and demo environment.\n##How did it benefit your career as well as your community?\nMy VCP-DCV ultimately set me on a path towards achieving my VCDX-NV last year. In between those two, there\u0026rsquo;s been a long stretch of passing the VCP-DCD and VCP-DCA exams and starting a job as a fulltime VMware consultant at a new employer ( ITQ ). Then, with the acquisition of Nicira, i learned that - even as a VMware consultant - networking was still in my blood, and i fasttracked myself towards doing more with NSX. I passed the VCP-NV relatively soon as it was released, went for the VCIX-NV, and as VMware NSX sales were finally gaining traction i managed to pass my VCDX-NV defense in Palo Alto a bit over half a year ago. These days, i\u0026rsquo;m mainly workin as a SDN architect - still for the same employer - and am regularly involved in NSX projects, both for VMware PSO as well as my own employer. I\u0026rsquo;m also active on the vExpert slack and like to help people out with questions related to NSX and provide advice to those that need it.\n##Knowing what you know today, what are some of the pain points in this certification that you can share with your audience?\nMy only real major issue with the VCP-DCV certification is the DCV-ICM training. While it is required to pass your first DCV, the skill levels required to pass the VCP-DCV are in no way relatable to the skills you learn during the ICM. While obviously you\u0026rsquo;ll need some real-life experience as well there is a noticeable discrepancy between the ICM and real life. If you\u0026rsquo;ve already got the skills to pass your VCP exam, the ICM is just an expensive hurdle on the way to become a first time VCP, and if you don\u0026rsquo;t have the hands-on experience the ICM doesn\u0026rsquo;t provide enough to pass the VCP-DCV. My experience with the ICM training for DCV is pretty dated though, so this may have changed, and i have noticed that VMware is providing much more guidance to pass your exam, including hands on labs, blueprints, documentation and mock exams.\n","date":"5 April 2017","permalink":"/posts/vmware-certification-first-steps/","section":"Posts","summary":"Full Disclosure: This blogpost is written for and part of the VMware vExpert NSX Certification blogging campaign.\nA long long time ago when i was still making my first steps in the wonderful world of IT, my role was that of the go-to operations guys for small-medium businesses.","title":"VMware Certifications - First steps"},{"content":"Currently i\u0026rsquo;m working on a NSX project that involves automation of NSX loadbalancers to provide loadbalancing as a catalog item. Now, while i would normally prefer deploying dedicated loadbalancers on-demand, a big constraint in this environment is the fact that the ISP takes a significant time to configure the GSLB for new services, so on-demand loadbalancers go straight out of the window. The second issue is that all SSL connections must be terminated on the backend, so we can\u0026rsquo;t do SSL offloading or reencryption.\nSo what we\u0026rsquo;re looking at now is loadbalancing traffic to multiple backends, based on the hostname of the request. The GSLB has been configured to forward any traffic intended for *.cloudapp.glsb.domain.tld towards a pool of NSX loadbalancers intended for all applications running on the cloud management platform.\nAs you may know, SSL passthrough doesn\u0026rsquo;t allow the inspection of the actual traffic. Fortunately, when using a \u0026ldquo;relatively\u0026rdquo; new browser or operating system (SNI was introduced in 2003), we now have support for SNI (Server Name Indication), which allows for multiple certificates on a shared IP. Before SNI was commonplace, SSL was the bane of shared hosting platforms as each HTTPS server required its own dedicated IP address. Nowadays, SNI is mostly used in the hosting world to run multiple ssl-backed applications on the same IP address. And as it turns out, HAProxy (which is used by the NSX loadbalancer) supports SNI inspection out of the box.\nSo as it turns out, loadbalancing SSL passthrough requests to different backends based on the SNI request is extremely easy:\nFirst off, we start with the regular configuration of the NSX loadbalancer. We\u0026rsquo;ll create a profile, set it to HTTPS, and ensure \u0026ldquo;Enable SSL Passthrough\u0026rdquo; is checked. Next, configure your Pools and virtual servers as you\u0026rsquo;d do normally, and finally create an application rule with the following content (replacing your backend pools and hostnames as applicable):\ntcp-request inspect-delay 5s tcp-request content accept if { req_ssl_hello_type 1 } tcp-request content reject use_backend HTTPS_pt_app1 if { req_ssl_sni -i app1.gslb.int.vxsan.com } use_backend HTTPS_pt_app2 if { req_ssl_sni -i app2.gslb.int.vxsan.com }\nAssign the application rule to your virtual server, and that\u0026rsquo;s all there is to it. Your loadbalancer should now correctly select the backend pool based on the hostname in the HTTPS handshake, allowing you to use multiple backends even with SSL passthrough.\nObviously, the tcp-request could be tuned for your own purposes (inspection delays can be significantly lower on LAN than on WAN, for example), and you could even do fancy things with wildcard or regex matching on the req_ssl_sni statement, but for the purpose of this blog we\u0026rsquo;ve shown that SSL passthrough does not need to limit your ability to perform dynamic backend selection in NSX.\n","date":"30 March 2017","permalink":"/posts/multi-backend-ssl-passthrough-loadbalancing-with-nsx/","section":"Posts","summary":"Currently i\u0026rsquo;m working on a NSX project that involves automation of NSX loadbalancers to provide loadbalancing as a catalog item. Now, while i would normally prefer deploying dedicated loadbalancers on-demand, a big constraint in this environment is the fact that the ISP takes a significant time to configure the GSLB for new services, so on-demand loadbalancers go straight out of the window.","title":"multi-backend SSL Passthrough Loadbalancing with VMware NSX"},{"content":"","date":"25 January 2017","permalink":"/tags/puppet/","section":"Tags","summary":"","title":"Puppet"},{"content":"","date":"25 January 2017","permalink":"/categories/puppet/","section":"Categories","summary":"","title":"Puppet"},{"content":"TL;DR: if you\u0026rsquo;re just interested in how to configure orchestrator to allow puppetDB querying, follow the link .\nPuppetDB is an extension to Puppet allowing you to aggregate data from Puppet nodes such as custom facts, catalogs and resources. While PuppetDB is installed by default in Puppet Enterprise, it is not included by default in Puppet Open Source. Installing and configuring PuppetDB - either on your puppet master or as a standalone server is as simple as following https://docs.puppet.com/puppetdb/4.3/install_from_packages.html .\n###PuppetDB and vRealize Automation\nNow that we have PuppetDB, what can we use it for related to vRealize automation? Some examples are:\n####Performing day two operations using information about the guest operating system:\nInstead of having to run commands through ssh or powershell to retrieve this information, you can now query the PuppetDB for this information. Not only is this faster, it\u0026rsquo;s also more reliable and doesn\u0026rsquo;t depend on the guest being up and running. An example of this would be setting the backup schedule based on the node\u0026rsquo;s environment, the application owner or the region the VM is deployed in.\n####Querying information of other nodes for the purpose of deployment.\nLet\u0026rsquo;s say you\u0026rsquo;re scaling out an application and need to perform post-provisioning tasks. Instead of storing the task-specific information in vRealize Orchestrator configuration elements or vRealize automation custom properties, you can now query Puppet directly for the required information. ####Best of all\nYou get to use a proper, clean API that returns standardized JSON instead of having to mess about with shell scripts that may or may not return correctly formatted information.\nSome interesting examples of information that can be used to improve your workflows:\nThe name of your nics in the guest operating system: vRealize automation only shows you the nic index, not the nic name. Since new versions of most Linux operating systems use bios device names for their nics, this has caused issues for all of us when workflows depend on static naming such as eth0, eth1, etc. Mountpoints of filesystems and their actual sizes. Instead of having to grep your /etc/mtab for the purpose of day 2 operations involving the filesystem, you can now do a simple query on the guest\u0026rsquo;s filesystem and parse that information. Whether or not a specific package or piece of software is installed, and if so - what version. Relationships between nodes: You can now see which node depends on other nodes, allowing you to improve dependency handling between multimachine nodes. In addition, you can create custom facts - either static or dynamic - to include in PuppetDB. ###Configuring vRealize Orchestrator\nSo how do we set all of this up? First, we have to start with signing a custom certificate for vRealize Orchestrator. As PuppetDB performs client-based certificate authentication, we need to provide vRealize Orchestrator with a certificate.\nStart off with generating a custom certificate on the puppetmaster. This assumes that your Puppetmaster is a CA. If not, you\u0026rsquo;ll have to find out how your certificates are generated and trusted):\nRun the following on your puppetmaster (replacing your.vro.fqdn with your actual vro machine\u0026rsquo;s fqdn. If you have a cluster of vRO machines each machine needs to have their own individual certificate generated:\npuppet cert generate your.vro.fqdn\nThis generates the following files:\n/etc/puppetlabs/puppet/ssl/certs/your.vro.fqdn.pem /etc/puppetlabs/puppet/ssl/private_keys/your.vro.fqdn.pem Next, we\u0026rsquo;ll need to generate a certificate file containing the private key and the full chain to import into vRO. This is important, as the vRO certificate store is very picky about the format of the file:\nrun the following command on your puppetmaster you signed the certificate on:\ncd /etc/puppetlabs/puppet/ssl cat private_keys/your.vro.fqdn.pem certs/your.vro.fqdn.pem certs/ca.pem \u0026gt; /tmp/vro-cert.pem Your certificate chain should now be located in /tmp/vro-cert.pem.\nNext, open orchestrator and perform the following:\nRun the Library\\Configuration\\Keystores\\Add key workflow. Select the keystore you want to add the key to (i created a separate keystore through the Library\\Configuration\\Keystores\\Create a keystore workflow, though this is entirely optional.) enter a key alias. This is entirely freeform, but i recommend providing a sensible alias. Paste the entire contents of the /tmp/vro-cert.pem file in the PEM encoded key field. Ensure that you have no whitespace in your head or tail. Leave the key password field blank, as we don\u0026rsquo;t have a password set. Afterwards, you can go to the inventory, and the vRO configuration\\Keystores should show your certificate with a padlock icon, indicating this certificate has a private key stored.\nNext, we\u0026rsquo;ll run the HTTP-REST\\Configuration\\Add a REST Host workflow. Enter the usual properties such as the name, URL, and certificate acceptance properties. Set the authentication in 2a to NONE, as we will not be performing credential-based authentication.\nThen, in the 4a SSL page, set \u0026ldquo;Verify whether the target hostname matches the names stored inside the server\u0026rsquo;s X509 certificates to yes. As puppet self-signs its certificates this should always match. Next, Select your certificate in the PrivateKey entry:\nWhen you run the workflow, you should now have a REST host added to your inventory. Note that if the certificate authentication fails, for whatever reason vRO will still add the rest host, but prepend its name with [Invalid]. This is most likely due to the connection succeeding, but vRA never receiving valid HTTPS data. This is what cost me some time to figure out why it wasn\u0026rsquo;t actually working.\nNext, you can run your REST queries as normal in your workflow. Some samples are located in the \u0026ldquo;HTTP-REST Samples\u0026rdquo; Folder for inspiration. As an example, this query returns the uptime fact for a specific node. You can obviously turn this into any query you need, and you\u0026rsquo;ll probably want to make the query URL dynamic:\nAnd the result is as follows:\n[{ \u0026#34;certname\u0026#34;:\u0026#34;media.int.vxsan.com\u0026#34;, \u0026#34;name\u0026#34;:\u0026#34;uptime\u0026#34;, \u0026#34;value\u0026#34;:\u0026#34;35 days\u0026#34;, \u0026#34;environment\u0026#34;:\u0026#34;production\u0026#34; }] And that\u0026rsquo;s all there is to querying your puppetDB dynamically from vRealize Orchestrator, and obviously if you have any other product that performs mutual certificate authentication you can use the same method.\nHappy puppeting!\n","date":"25 January 2017","permalink":"/posts/querying-puppetdb-with-vrealize-orchestrator-and-vrealize-automation/","section":"Posts","summary":"TL;DR: if you\u0026rsquo;re just interested in how to configure orchestrator to allow puppetDB querying, follow the link .\nPuppetDB is an extension to Puppet allowing you to aggregate data from Puppet nodes such as custom facts, catalogs and resources.","title":"Querying PuppetDB with vRealize Orchestrator and vRealize Automation"},{"content":"As you may know, vRealize Automation 7 brought us a great deal of improvements, one of which is the composite blueprint and the software components. With the new release of vRA 7.2 i\u0026rsquo;ve performed a full redeploy in my lab environment and - since we already have a full puppet stack running - decided to integrate the two.\nWhy puppet? # Because, let\u0026rsquo;s be honest: vRealize automation still isn\u0026rsquo;t the way to perform advanced application configuration, manage configuration drift, and ensure versioning. vRealize Automation enables you to significantly simplify the deployment of both single virtual machines as well as full multitier application stacks, but where it still lacks is in actual software configuration and state change management. So why not have our cake and eat it too, by using vRealize Automation for the virtual machine and infrastructure deployment, and Puppet for the operating system and application deployment?\nInstalling the Puppet environment # I\u0026rsquo;m not going to go through the entire setup as there are numerous blogs detailing this much better than i could ever do. Instead, i\u0026rsquo;ll give you a quick overview of my puppet environment.\nCurrently everything is running on the FOSS Puppet stack, but the examples used below should work equally well with Puppet Enterprise. In fact, because Puppet Enterprise provides you with the PuppetDB and its API and a preconfigured MCollective stack, using Puppet Enterprise would make some things even easier such as querying specific information directly from puppet through vRealize Orchestrator.\nFor our lab, we are running a single Puppet master with the following tools:\ngit r10k hiera We have two environments in r10k: production and staging. While staging isn\u0026rsquo;t used too often as it\u0026rsquo;s only my home machines and lab, i prefer having it around as it enforces best practice.\nThe directory structure used for my puppet code repo is as follows:\n`repo\ndata os roles common.yaml manifests site.pp site profile role hiera.yaml ` hiera.yaml is defined as follows:\n--- version: 4 datadir: data hierarchy: - name: \u0026#34;Operating System\u0026#34; backend: yaml path: \u0026#34;os/%{::operatingsystem}\u0026#34; - name: \u0026#34;Roles\u0026#34; backend: yaml path: \u0026#34;roles/%{::role}\u0026#34; - name: \u0026#34;Nodes\u0026#34; backend: yaml path: \u0026#34;nodes/%{::trusted.certname}\u0026#34; - name: \u0026#34;common\u0026#34; backend: yaml The site contains all roles and profile manifests. For example, this is my mediaserver manifest:\nclass role::mediaserver { include profile::transmission include profile::packages include profile::base_linux include profile::base include profile::cron include profile::nfs include profile::apache } And the data/roles/mediaserver.yaml contains the following:\n--- classes: - \u0026#39;role::mediaserver\u0026#39; cron::job: \u0026#39;transmission_cleanup\u0026#39;: command: \u0026#39;/opt/scripts/removecompletedtorrent.sh\u0026#39; description: \u0026#39;cleans up old transmission files\u0026#39; minute: \u0026#39;0\u0026#39; hour: \u0026#39;3\u0026#39; weekday: \u0026#39;1\u0026#39; nfs::mount: \u0026#39;/mnt/media\u0026#39;: device: \u0026#39;172.22.100.1:/mnt/tank/uncompressed/media\u0026#39; fstype: \u0026#39;nfs\u0026#39; ensure: \u0026#39;mounted\u0026#39; options: \u0026#39;auto,nofail,noatime,nolock,intr,tcp,actimeo=1800\u0026#39; remounts: true atboot: true \u0026#39;/mnt/media-staging\u0026#39;: device: \u0026#39;172.22.100.1:/mnt/tank/uncompressed/media-staging\u0026#39; fstype: \u0026#39;nfs\u0026#39; ensure: \u0026#39;mounted\u0026#39; options: \u0026#39;auto,nofail,noatime,nolock,intr,tcp,actimeo=1800\u0026#39; remounts: true atboot: true transmission::rpc_username: \u0026#39;transmission\u0026#39; transmission::rpc_password: \u0026#39;nothingtoseeheremovealong\u0026#39; transmission::rpc_port: \u0026#39;9091\u0026#39; transmission::peer_port: \u0026#39;51413\u0026#39; transmission::blocklist_url: \u0026#39;http://john.bitsurge.net/public/biglist.p2p.gz\u0026#39; transmission::download_dir: \u0026#39;/mnt/media-staging/transmission\u0026#39; transmission::incomplete_dir: \u0026#39;/mnt/media-staging/incomplete\u0026#39; transmission::encryption: \u0026#39;1\u0026#39; transmission::dht_enabled: true transmission::pex_enabled: true transmission::utp_enabled: false apache::vhosts: \u0026#39;repo\u0026#39;: port: \u0026#39;80\u0026#39; docroot: \u0026#39;/var/www/repo\u0026#39; servername: \u0026#39;repo.int.vxsan.com\u0026#39; If we look at the hiera.yaml above, this setup allows me to completely deploy and configure a server with this role by only setting the roles fact.\nNow fortunately puppet includes the amazing tool facter which - in addition to getting OS generated facts - allows you to set custom facts. The only thing we need to do for that is provision a file containing these custom facts. In our case, as this is a static fact we\u0026rsquo;ve deployed a file called /etc/puppetlabs/facter/facts.d/role.yamlcontaining the following:\n--- role: mediaserver And that\u0026rsquo;s the only thing we need to do: from now on the fact \u0026ldquo;role\u0026rdquo; will be set to \u0026ldquo;mediaserver\u0026rdquo; for this server.\nvRealize Automation setup # Now onto the juicy bits: Deployment of this application through vRealize Automation.\nWe\u0026rsquo;ve created a basic single machine blueprint with some custom property definitions to consume in puppet:\nObviously you should be getting those environments and roles through external values as opposed to static values stored in vRA, but for the purpose of this demo static values will suffice.\nThe final request looks as follows:\nThe user can select his environment and the puppet role during request time. Now on to the actual blueprint.\nTo consume the Puppet environment and role, we\u0026rsquo;ll be using the vRealize Automation Software components. As these are only included in the Enterprise license you might not have these, but don\u0026rsquo;t worry: What we\u0026rsquo;re doing here can be done without the enterprise license, through either vRealize Orchestrator workflows by running programs in guest or through the vRealize Automation agent. While the implementation might be slightly different, the end result is the same.\nWe\u0026rsquo;ve created a single software component to install the Puppet agent, configure it and perform a first run. These are the properties we\u0026rsquo;ll be using:\nPuppet_software_package is a hardcoded content property to download the agent from the puppetlabs repository and install it. The other two properties you\u0026rsquo;ve already seen in the description above.\nOut software components will contain the following scripts:\n###install\n#!/bin/bash dpkg -i $puppet_software_package apt-get update apt-get install puppet-agent ###Configure\n#!/bin/bash ## Set some variables conf=\u0026#39;/etc/puppetlabs/puppet/puppet.conf\u0026#39; role=\u0026#39;/etc/puppetlabs/facter/facts.d/role.yaml\u0026#39; ## remove original puppet agent rm -f $conf ## write a new puppet.conf touch $conf cat \u0026lt;\u0026lt; EOT \u0026gt;\u0026gt; $conf [main] use_srv_records = true srv_domain = int.vxsan.com environment=$puppet_agent_environment EOT ## set our role fact cat \u0026lt;\u0026lt; EOT \u0026gt;\u0026gt; $role --- role: $puppet_facts_role EOT ###Start\n#!/bin/bash systemctl start puppet systemctl enable puppet And our final blueprint in vRealize Automation will look as follows:\nAnd that\u0026rsquo;s it. With the new software components, it really is that simple. No more uploading scripts to the guest through vRO, no more having to manually troubleshoots scripts or installing the software on your own. Instead, play on the strengths of puppet to deploy and configure your software, and the strengths of vRealize automation to deploy and configure your virtual machines and infrastructure.\nNow that we\u0026rsquo;ve done the basic puppet deployment, consider what else you can do through this. Some examples of things that can be done:\nSet the role fact as a static for each individual machine for composite blueprints with multiple machines to provide automatically deployed multitier applications. Create day two operations to set facts on the puppet managed machines. Determine facts such as environments, teams, datacenters, locations, etc. based on properties passed by vRealize Automation. Imagine deploying business-group or even requestor-specific configurations such as ssh keys, allowed users, etc. When using PuppetDB, vRealize Orchestrator can be used instead of software components to classify nodes, set custom facts and perform registration. Happy puppeting!\n*PS: The attentive reader will have noticed that i haven\u0026rsquo;t signed the client certificate: i cheated a bit here and set up autosigning, which obviously a big no-no in production. Creating a vRealize Orchestrator worfklow to sign the certificate isn\u0026rsquo;t too complicated, but that\u0026rsquo;s something for next time. *\n","date":"19 January 2017","permalink":"/posts/vm-configuration-with-puppet-and-vrealize-automation-7/","section":"Posts","summary":"As you may know, vRealize Automation 7 brought us a great deal of improvements, one of which is the composite blueprint and the software components. With the new release of vRA 7.","title":"VM deployment \u0026 configuration with Puppet and vRealize Automation 7"},{"content":"As i was working with some of the new puppet 4 functions, i noticed that most documentation for node classification still refers to hiera_include for the purpose of node classification and class inclusion. However, as there are some issues with the old hiera format and for the purpose of removing dependence on specific backends, puppet 4 now also supports a new hiera.yaml format. This new format allows for some great features such as mixing backends per hierarchy source and per-environment hiera.yaml files, which is great if you\u0026rsquo;re using r10k for multiple non-homogeneous environments. Using this new hiera format does require one to classify nodes differently.\nPreviously, one would use hiera_include('classes') in your site.pp, which doesn\u0026rsquo;t work with a per-environment hiera.yaml. The replacement for this is as simple as adding lookup('classes', Array[String], 'unique').include to your site.pp, allowing you to loop over your hiera-defined array of classes and include then in your node. The unique merge behavior will merge all lookups of classes into a single flat array, which is what you\u0026rsquo;d want to add classes from multiple hierarchies in hiera.\nIn addition, the lookup function supports some interesting features such as knockout prefixes which allows you to remove results from the resulting set of hiera data. While usually this\u0026rsquo;d be the result of a flawed hierarchy, sometimes - for whatever reason - you just need to override settings on specific hosts\nAs an example, assume you\u0026rsquo;d have the following two node classifications for sets of servers to allow specific groups access through ssh:\n#managed.yaml ssh::server::groupallow - team::administrators - team::operators - team::security and\n#unmanaged.yaml ssh::server::groupallow - team::administrators - team::operators - team::developers - team::security Now, if you\u0026rsquo;d want to override access for the operators team on a very specific set of servers, previous one would have to use a separate class of excluded groups and substract that from the resulting set in hiera.\nWith puppet 4 lookup however, we can use the knockout-prefix to remove specific results from the resulting set. Let\u0026rsquo;s assume we have a specific group of servers which - for all intents and purposese - are part of the unmanaged group of servers, but have specific requirements to not allow daily operations access to these machines. We\u0026rsquo;d use the following hiera for this set of servers\n#unmanaged-confidential.yaml - !!team::operators and the following lookup:\nlookup({ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;groupallow\u0026#39; \u0026#39;merge\u0026#39; =\u0026gt; { \u0026#39;strategy\u0026#39; =\u0026gt; \u0026#39;deep\u0026#39;, \u0026#39;knockout_prefix\u0026#39; =\u0026gt; \u0026#39;!!\u0026#39;, }, }) The resulting set of classes for our snowflake servers would become:\nssh::server::groupallow - team::administrators - team::developers - team::security Keep in mind the puppet lookup function is still experimental, but as far as i can see this would be the way forward to provide backend-agnostic data lookup and classification. For more information on puppet lookup, see https://docs.puppet.com/puppet/latest/lookup_quick.htm . Happy puppeting!\n","date":"3 January 2017","permalink":"/posts/puppet-4-lookup-hiera-node-classification/","section":"Posts","summary":"As i was working with some of the new puppet 4 functions, i noticed that most documentation for node classification still refers to hiera_include for the purpose of node classification and class inclusion.","title":"Puppet 4 lookup \u0026 hiera node classification"},{"content":"","date":"17 November 2016","permalink":"/tags/powercli/","section":"Tags","summary":"","title":"PowerCLI"},{"content":"","date":"17 November 2016","permalink":"/categories/powercli/","section":"Categories","summary":"","title":"PowerCLI"},{"content":"People that know me might be aware of the fact that i\u0026rsquo;m a strong proponent of using powerCLI, due to its power of allowing the simple yet powerful automation of single tasks (you should be using Orchestrator for recurring tasks, but even that can use powerCLI) as well as the great runbook scripting capatibility for new deployments, upgrades, migrations and disaster recovery. Combined with tools such as Pester , it allows for the automation of certain tasks without requiring windows admins familiar with PowerShell to relearn their skills.\nNow, freshly released, PowerCLI 6.5R1 brings some great improvements to an already amazing tool. Aside of the general performance and stability fixes, there are a few new features i want to highlight:\n#####Cross-SSO vMotion with Move-VM\nWhile this was already named in the release post, this one is severly underexposed. Move-VM has been updated to allow the vMotion of virtual machines between vCenters in a single SSO domain, as well as the vMotion of virtual machines between SSO domains. This was something that was previously available through the API only, and can now be done through powerCLI. This essentially turns your migration plan into get-vm | move-vm. It also allows for the vMotion between separate hosting environments, online onboarding of virtual machines by cloud providers, and various other possibilities.\n#####vDisk management\nAs opposed to the old way of powerCLI, disks were managed by modifying the VM hardware, which was a relatively complicated way to manage this. PowerCLI 6.5 adds the feature to manage virtual machine disks directly through the *-VDisk cmdlets, allowing for significantly easier adding, removal and modification of vDisks, in addition to allowing the scripted copy of individual virtual disks. This would allow you to easily clone only a specific disk (such as a data disk) of a virtual machine\u0026rsquo;s disk for the purpose of upgrade tests, data forensics and dev/test environments.\n#####Storage policy management\nStorage policies were one of those things that was still mostly manually managed since it was relatively hard to script. While this wasn\u0026rsquo;t a big issue for most organizations, the storage policy cmdlets allow for the classification of disks provisioned to your vSphere environment as well as advanced vDisk management and automation (see the section above). Imagine being able to automatically add a disk to a virtual machine in the correct storage tier with nothing more than a oneliner of piped commands.\nAnd last but not least\n#####VSAN Management\nVSAN can be fully managed through PowerCLI now, so you never have to touch the maze known as the VSAN management interface ever again. That in itself should be reason enough to upgrade to powerCLI 6.5R1.\nThe user guide and changelog can respectively be found at https://www.vmware.com/support/developer/PowerCLI/PowerCLI65R1/doc/vmware-powecli-65r1-user-guide.pdf and https://www.vmware.com/support/developer/PowerCLI/changelog.html#PowerCLI65R1 , and ofcourse the download at https://my.vmware.com/web/vmware/details?downloadGroup=PCLI650R1\u0026productId=614 .\n","date":"17 November 2016","permalink":"/posts/powercli-6-5r1-brings-some-amazing-new-automation-features/","section":"Posts","summary":"People that know me might be aware of the fact that i\u0026rsquo;m a strong proponent of using powerCLI, due to its power of allowing the simple yet powerful automation of single tasks (you should be using Orchestrator for recurring tasks, but even that can use powerCLI) as well as the great runbook scripting capatibility for new deployments, upgrades, migrations and disaster recovery.","title":"PowerCLI 6.5R1 brings some amazing new automation features"},{"content":"As part of a microsegmentation project, we\u0026rsquo;re also doing the full implementation of the DFW ruleset to isolate and categorize application tiers and inter-tier traffic. Since implementing these rules manually would be very labour-intensive and the risk of manual errors are always lurking, the obvious way would be to automate this. Fortunately the security model lends itself to the minimisation of rules and consistent rulesets.\nAn example of a few of the scripts used are posted below as a simple reference. Obviously these are samples only to be used for inspiration, and while one could write an entire script to parse a preexisting csv or json to do a fully automated deployment, for now this is my quick and dirty way to get a full DFW deployment done in seconds with PowerNSX.\n$secTags = @( \u0026#34;DTAP:Develop\u0026#34;, \u0026#34;DTAP:Test\u0026#34;, \u0026#34;DTAP:Acceptance\u0026#34;, \u0026#34;DTAP:Production\u0026#34;, ) $secTags |% { New-NsxSecurityTag -name $_ } $secGroups = @( \u0026#34;DTAP:Develop\u0026#34;, \u0026#34;DTAP:Test\u0026#34;, \u0026#34;DTAP:Acceptance\u0026#34;, \u0026#34;DTAP:Production\u0026#34;, ) $secGroups |% { $secTag = get-NSXSecurityTag -Name $_ $secGroup = New-NsxSecurityGroup -name $_ if ($secTag) { get-NSXSecurityGroup $secGroup | add-NSXSecurityGroupMember -Member $secTag } } $secGroups = @( \u0026#34;DTAP:Develop\u0026#34;, \u0026#34;DTAP:Test\u0026#34;, \u0026#34;DTAP:Acceptance\u0026#34;, \u0026#34;DTAP:Production\u0026#34;, ) $secGroups |% { $secGroup = Get-NsxSecurityGroup -name $_ Get-NsxFirewallSection \u0026#34;DTAP Rules\u0026#34; | New-NsxFirewallRule -Name \u0026#34;Deny all from $_ to NOT $_\u0026#34; -Source $secGroup -Destination $secGroup -NegateDestination -Action reject -EnableLogging -Tag $_ Get-NsxFirewallSection \u0026#34;DTAP Rules\u0026#34; | New-NsxFirewallRule -Name \u0026#34;Deny all from NOT $_ to $_\u0026#34; -Source $secGroup -Destination $secGroup -NegateSource -Action reject -EnableLogging -Tag $_ } Future improvements: Creating firewall rules, security groups and its members through a JSON object.\n","date":"14 November 2016","permalink":"/posts/automating-distributed-firewall-rule-deployment-with-powernsx/","section":"Posts","summary":"As part of a microsegmentation project, we\u0026rsquo;re also doing the full implementation of the DFW ruleset to isolate and categorize application tiers and inter-tier traffic. Since implementing these rules manually would be very labour-intensive and the risk of manual errors are always lurking, the obvious way would be to automate this.","title":"Automating distributed firewall rule deployment with PowerNSX"},{"content":"While the subject has come up quite a bit, it\u0026rsquo;s still mostly shrouded in mystery and getting a proper answer on what does and doesn\u0026rsquo;t work usually requires some shady voodoo rituals. SO let\u0026rsquo;s talk about dynamic routing over VPC.\nFirst of all, let\u0026rsquo;s get one misconception out of the way: dynamic routing through a VPC has always been supported. This means that your VPC chassis only serves as a L2 transport for your dynamic routing traffic. As seen in the picture below, routing between L3-A and L3-B/L3-C is always supported since loop protection never kicks in.\nWhat is important however is that cisco has changed their stance on dynamic routing over VPC from \u0026ldquo;not supported, go away\u0026rdquo; to \u0026ldquo;it\u0026rsquo;s supported, but with very specific constraints. This was actually changed a bit over a year ago but apparently a lot of people still don\u0026rsquo;t know this. A network integrator i was working with on NSX wasn\u0026rsquo;t aware of this, and it seems more people - even those that work with Nexus VPC\u0026rsquo;s daily - aren\u0026rsquo;t.\nSo what are the constraints? If we look at http://www.cisco.com/c/en/us/support/docs/ip/ip-routing/118997-technote-nexus-00.html , we can see that dynamic routing between Nexus-A and Nexus-B is very dependent on what model you have. If this is what you want, your best bet is to go with a 9k or 7k, which is usually where you\u0026rsquo;d want dynamic routing in a VPC setup as they\u0026rsquo;ll most likely be located in your DC core or aggregation. Nexus 3k/3.5k also support everything under the sun, the really odd ones out are the 5k and 6k\u0026rsquo;s which support dynamic routing over their VPC peer link only, which is obviously a limitation.\nNow when we look at dynamic routing between L3-A and Nexus-B, we\u0026rsquo;ll see that this is where we\u0026rsquo;ll slowly start going into a hazy fog of compatibility issues. It is entirely dependent on your model whether or not dynamic routing from L3-A to Nexus-B. The most important thing out of this page to realise though is that \u0026ldquo;L3-A to Nexus-A peering is always supported for L2/L3.\u0026rdquo;.\nNow what does this all have to do with NSX? If we look at the NSX Edge deployment model, you\u0026rsquo;ll usually peer those with your aggregation or core. So in this image your ESG would be L3-A and your physical peers would be Nexus-A and Nexus-B. Should be all fine if you have an L2 connection between your edge cluster and your physical switches, right?\nNot exactly. Because the cisco document clearly states that this applies to physical L2 connections only. A virtual L2 connection would be a VLAN with a SVI configured, and as such would be traversing the VPC configured on your access switches to the aggregation/core switches and would be subject to loop prevention protocols in the VPC.\nSo in general, while Cisco does have better support for dynamic routing over VPC, i\u0026rsquo;d personally recommend either building a non-VPC network for dynamic routing - which in turn is a good argument for a dedicated edge cluster - or doing dynamic routing to non-VPC cores, so you\u0026rsquo;d get something like this: Earlier this week we\u0026rsquo;ve had a good discussion about how to perform routing, and in the customer\u0026rsquo;s case they have 7k\u0026rsquo;s as DC cores with a new N9k ACI fabric below that, and just the headaches saved from not having to worry about VPC configuration and restrictions to specific firmware versions and types of switches made us decide to just plug in additional cables, create non-VPC VLANs and just live with the fact that VPC and dynamic routing is still a bit of a mess.\nFortunately we have dedicated border leafs connected to the 7k cores, which means we only need to configure the non-VPC VLANs on the border leaf, saving a lot of work. However, if you have a classic access/aggregation/core network you\u0026rsquo;ll need to pay very careful attention to which switches your edges will peer with and which vpc\u0026rsquo;s you will traverse.\nUnfortunately the VMware documentation regarding dynamic routing and VPC\u0026rsquo;s don\u0026rsquo;t take the document linked above into account, so while it\u0026rsquo;s still unclear what is and isn\u0026rsquo;t supported, my personal preference is to always use dedicated non-vpc links for your routing networks, and hopefully Cisco and VMware will sort out what exactly is and isn\u0026rsquo;t supported regarding VPC\u0026rsquo;s and routing.\n","date":"16 August 2016","permalink":"/posts/nsx-dynamic-routing-and-vpc/","section":"Posts","summary":"While the subject has come up quite a bit, it\u0026rsquo;s still mostly shrouded in mystery and getting a proper answer on what does and doesn\u0026rsquo;t work usually requires some shady voodoo rituals.","title":"NSX, Dynamic routing and VPC"},{"content":"In most of my projects, powershell and its vendor-specific modules such as VMware\u0026rsquo;s powercli is usually a necessity to work effectively. In addition, most customers appreciate it greatly if you can leave them with a handful of examples on how to run standard procedures through a script, even though it might not always be part of the deliverables.\nMost of the time however, those scripts are written as one-offs, tend to require major rewriting for a different use case and don\u0026rsquo;t have any kind of error handling or input validation. As such, i usually tend to rewrite a new script from scratch for a new project, which kind of defeats the purpose of the reusability of scripts.\nLast week i sat down and decided to think up a proper development environment for powershell related tasks. While it\u0026rsquo;s only a beginning, it does help with writing clean code allowing you to spend less time coding the next time.\nFor my current setup i\u0026rsquo;m using the following tools:\nVisual studio code and the Visual studio powershell extension - As powerGUI has really reached the end of the line as far as support goes, i had to find another tool. Normally i\u0026rsquo;m used to things like sublime text for MacOS which is an amazing editor, but unfortunately the powershell input is quite limited. Pester - A great testing framework for powershell that enables you to perform test-driven development on powershell. Unfortunately, you won\u0026rsquo;t have the time for this in each project and if a script is truly one-off you\u0026rsquo;d have to question the added benefits of TDD, but in general i do try to write a test for each script. PS script analyzer - A wonderful set of linting tools for powershell. In my opinion, everyone should be using this, as it allows you to write clean, standardized code instead of unique snowflakes. It also integrates with VS.Code and you can use it in unit tests in Pester. Bitbucket - Saving all your scripts in a version control is the most essential part of proper coding. Without version control you\u0026rsquo;ll end up with scripts all over the place, various versions modified for each use case, and you\u0026rsquo;ll usually lose track of that \u0026ldquo;where did i place that?\u0026quot;-script you desperately need. Using version control also forces you to write reusable code because you don\u0026rsquo;t want to end up with a large amount of branches for each use case. A powershell template. Using a template file allows you to standardize parameters, help functions, and more. By using a template you\u0026rsquo;ll create the same file over and over again, as well as making it easier for you to write clean code since filling out the supporting comments and information will be significantly easier. For my current template i\u0026rsquo;m using a template based on the powershell template from 9to5it.com . Below is an example of an empty script i\u0026rsquo;m using to automate the management of IP sets in NSX.\n#requires -version 4 \u0026lt;# .SYNOPSIS This script enables the creation of NSX IP sets based on information from a csv file. .DESCRIPTION This script enables the creation of NSX IP sets based on information from a csv file. .PARAMETER csv This parameter sets the CSV file from which to import your NSX IP sets. It should be in the following format: name;ipobject1,ipobject2,ipobjectn;description .PARAMETER nsxmanager This parameter sets the hostname of the NSX manager used to connect to your NSX environment. .INPUTS None .OUTPUTS Log File The script log file stored in C:\\Windows\\Temp\\create-ipsets.log .NOTES Version: 1.0 Author: Sjors Robroek Creation Date: 14-06-2016 Purpose/Change: Initial script development .EXAMPLE import-ipset -nsxmanager nsxm.domain.tld -csv C:\\temp\\ipsets.csv #\u0026gt; #---------------------------------------------------------[Initialisations]-------------------------------------------------------- #Set Error Action to Silently Continue $ErrorActionPreference = \u0026#39;stop\u0026#39; #Import PSLogging Module Import-Module PSLogging #----------------------------------------------------------[Declarations]---------------------------------------------------------- #Script Version $sScriptVersion = \u0026#39;1.0\u0026#39; #Log File Info $sLogPath = \u0026#39;C:\\Windows\\Temp\u0026#39; $sLogName = \u0026#39;create-ipsets.log\u0026#39; $sLogFile = Join-Path -Path $sLogPath -ChildPath $sLogName #-----------------------------------------------------------[Functions]------------------------------------------------------------ Function import-ipset { Param ( [Parameter(Mandatory=$True, ValueFromPipeline=$True, ValueFromPipelineByPropertyName=$True, HelpMessage=\u0026#39;please enter the path to your CSV file. \u0026#39;)] [validatescript({ test-path $_ })] [string]$csv , [Parameter(Mandatory=$True, ValueFromPipeline=$True, ValueFromPipelineByPropertyName=$True, HelpMessage=\u0026#39;Please enter the NSX manager FQDN\u0026#39;)] [validateScript({ (resolve-dnsname -dnsonly -name $_) -ne $null })] [string] $nsxmanager ) Begin { Write-LogInfo -LogPath $sLogFile -Message \u0026#39;\u0026lt;description of what is going on\u0026gt;...\u0026#39; } Process { Try { } Catch { Write-LogError -LogPath $sLogFile -Message $_.Exception -ExitGracefully write-error hallo Break } } End { If ($?) { Write-LogInfo -LogPath $sLogFile -Message \u0026#39;Completed Successfully.\u0026#39; Write-LogInfo -LogPath $sLogFile -Message \u0026#39; \u0026#39; } } } #-----------------------------------------------------------[Execution]------------------------------------------------------------ \u0026lt;# Start-Log -LogPath $sLogPath -LogName $sLogName -ScriptVersion $sScriptVersion #Script Execution goes here Stop-Log -LogPath $sLogFile #\u0026gt; Using PS script analyzer for linting # Using PS script analyzer is actually as simple as running Invoke-ScriptAnalyzer in the directory where you want to analyzer scripts. There are quite some other options, such as ignorning specific rules, running only a specific script, and writing your own rules, all of which can be found in the PSscriptAnalyzer documentation .\nUsing pester for test-driven development # When you want to do true test-driven development, your code should always be defined a test first. Chances are, if you can\u0026rsquo;t write a test for it, you probably don\u0026rsquo;t understand what you want your code to do exactly.\nFirst you\u0026rsquo;d need to install pester. Fortunately, if you have a recent version of powershell or nuget installed, installing pester is as simple as running Install-Package Pester\nAfter we\u0026rsquo;ve created our powershell script and functions, we\u0026rsquo;ll create the same file with tests appended to it. So if your file is called configure-ha.ps1, your pester file would be called configure-ha.tests.ps1.\nLet\u0026rsquo;s say we want to test the HA status of our cluster. A simple test in pester should look like this. Since pester will need access to the configuration in your scripts, we\u0026rsquo;ll need to use global variables unfortunately. Make sure that your script sets the global variable if needed, or prompt for them in pester.\n$here = Split-Path -Parent $MyInvocation.MyCommand.Path $sut = (Split-Path -Leaf $MyInvocation.MyCommand.Path).Replace(\u0026#34;.Tests.\u0026#34;, \u0026#34;.\u0026#34;) . \u0026#34;$here\\$sut\u0026#34; Describe \u0026#39;Cluster has HA configured\u0026#39; { It \u0026#34;Tests if cluster has HA enabled\u0026#34; { (get-cluster $global.cluster).HAEnabled | Should be $true } It \u0026#34;Tests if HA isolation response is set to PowerOff { (get-cluster $global.cluster).HAisolationResponse | Should be \u0026#34;PowerOff\u0026#34; } } To actually run the tests, we\u0026rsquo;ll need to invoke pester in the directory your scripts are located in. Run the command invoke-pester and it will start running all the unit test files it can find.\nAnd with only the above code, we\u0026rsquo;ve created a unit test that will test if your cluster has HA enabled, and if the isolation response is set correctly. Pester will parse your powershell file and run the tests as shown in the Describe block and give you a very concise output of all the tests ran. And this is only a very small sample of what pester can do for you to validate your code. If you\u0026rsquo;re the kind of person that regularly writes powershell code that others need to use, or if you want to start automating more daily operational tasks, i\u0026rsquo;d recommend taking a look at pester and setting up a proper development environment, even if you\u0026rsquo;re not a developer.\n","date":"18 June 2016","permalink":"/posts/setting-up-a-powershell-development-environment-for-non-programmers/","section":"Posts","summary":"In most of my projects, powershell and its vendor-specific modules such as VMware\u0026rsquo;s powercli is usually a necessity to work effectively. In addition, most customers appreciate it greatly if you can leave them with a handful of examples on how to run standard procedures through a script, even though it might not always be part of the deliverables.","title":"Setting up a powershell development environment for non-programmers"},{"content":"About me # Hi. I\u0026rsquo;m Sjors, the author of this blog and the person you\u0026rsquo;re currently reading about. I\u0026rsquo;ve been working in IT since 1999 in various functions, from support desk to frontend development, Cisco network engineering, Linux engineer and currently mainly focused on technical consultancy, specifically the design and implementation of VMware NSX. In addition, i like to stay up to date on security news and trends, which certainly helps in the networking world.\nCertifications # Currently, i hold the following relevant certifications worth mentioning:\nVMware VCDX #237, Network Virtualization VCP6-NV, VCP5-DVC, VCP5-IAAS VCAP5-DCD, VCAP5-DCA, VCIX6-NV VMware vExpert, vExpert NSX Cisco CCNA CCNP CCDP ARCH LPIC-1 \u0026amp; LPIC-2 MCITP-SA Various other certifications from vendors such as Trend Micro, Dell, etc. Why VXSAN.COM? # Because FCoTR is not nearly software-defined enough.\nDisclaimer # Ofcourse, no about page would be complete without a disclaimer. Everyone is free to do whatever they want with the information posted on this site, though a reference is always appreciated. Obviously, my opinion is my own and doesn\u0026rsquo;t reflect the official opinion of my employer or anyone else referenced. And since everyone reading this is a grown adult: If you take any information here for granted without doing your own research, you\u0026rsquo;re on your own. After all, why would you trust a random stranger on the internet?\n","date":"26 May 2016","permalink":"/posts/whoami/","section":"Posts","summary":"About me # Hi. I\u0026rsquo;m Sjors, the author of this blog and the person you\u0026rsquo;re currently reading about. I\u0026rsquo;ve been working in IT since 1999 in various functions, from support desk to frontend development, Cisco network engineering, Linux engineer and currently mainly focused on technical consultancy, specifically the design and implementation of VMware NSX.","title":"/usr/bin/whoami"},{"content":"/usr/bin/whoami # ","date":"1 January 0001","permalink":"/about/","section":"","summary":"/usr/bin/whoami # ","title":""},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"}]